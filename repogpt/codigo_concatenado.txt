# ==================== INICIO: ./__init__.py ====================
# repogpt/__init__.py

# Versión del paquete
__version__ = "0.2.0" # Actualizar según sea necesario

# Opcional: importar elementos clave para acceso directo
# from .analyzer import RepositoryAnalyzer
# from .exceptions import RepoGPTException

# Configurar un logger NullHandler por defecto para evitar mensajes si la app
# que usa la librería no configura logging.
import logging
logging.getLogger(__name__).addHandler(logging.NullHandler())
# ==================== FIN: ./__init__.py ======================

# ==================== INICIO: ./__main__.py ====================
# repogpt/__main__.py
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Union, Any

from repogpt.analyzer import RepositoryAnalyzer
from repogpt.utils.logging import configure_logging
from repogpt.extractors import base as extractor_base
# Asegúrate de que los extractores que necesitas estén aquí
from repogpt.extractors import dependencies, git, metrics, todos
from repogpt.reporting import base as reporter_base
from repogpt.reporting import json_reporter, markdown_reporter
from repogpt.exceptions import RepoGPTException # Importar excepción

logger = logging.getLogger(__name__)

# Extractores disponibles (puedes ajustar según sea necesario)
AVAILABLE_EXTRACTORS = {
    "dependencies": dependencies.DependencyExtractor,
    "git": git.GitInfoExtractor,
    "metrics": metrics.CodeMetricsExtractor,
    "todos": todos.TodoFixmeExtractor,
}

AVAILABLE_REPORTERS = {
    "json": json_reporter.JsonReporter,
    "md": markdown_reporter.MarkdownReporter,
}

def main():
    parser = argparse.ArgumentParser(
        description="RepoGPT: Analiza y resume repositorios de código para LLMs.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        "repo_path",
        nargs="?",
        default=".",
        type=Path,
        help="Ruta al repositorio a analizar."
    )
    parser.add_argument(
        "--start-path",
        default="",
        type=str,
        help="Subdirectorio relativo dentro del repositorio para iniciar el análisis."
    )
    parser.add_argument(
        "--output-file",
        "-o",
        default=None,
        type=Path,
        help="Archivo para guardar el reporte. Si no se especifica, se imprime en consola."
    )
    parser.add_argument(
        "--format",
        "-f",
        default="md", # Cambiado a Markdown por defecto, más útil para la estructura
        choices=AVAILABLE_REPORTERS.keys(),
        help="Formato del reporte de salida."
    )
    # Default extractors: quitar los que no sean esenciales para la estructura base
    # 'dependencies', 'git', 'metrics' son necesarios para las secciones opcionales
    # 'todos' es necesario para el conteo por archivo y la sección opcional
    parser.add_argument(
        "--extractors",
        default=",".join(AVAILABLE_EXTRACTORS.keys()), # Mantener todos por ahora
        help=(f"Extractores a usar, separados por coma. "
              f"Disponibles: {', '.join(AVAILABLE_EXTRACTORS.keys())}")
    )
    parser.add_argument(
        "--max-workers",
        type=int,
        default=4,
        help="Número máximo de hilos para procesar archivos."
    )
    parser.add_argument(
        "--max-file-size",
        type=int,
        default=2 * 1024 * 1024, # 2MB
        help="Tamaño máximo de archivo a procesar en bytes."
    )
    parser.add_argument(
        "--no-gitignore",
        action="store_true",
        help="No usar el archivo .gitignore para excluir archivos."
    )
    parser.add_argument(
        "--log-level",
        default="INFO",
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        help="Nivel de detalle del log."
    )
    parser.add_argument(
        "--version",
        action="version",
        version="RepoGPT 0.3.0" # Actualizar versión
    )

    # --- Nuevos Flags para Secciones Opcionales del Reporte ---
    parser.add_argument(
        "--summary",
        action="store_true",
        default=False,
        help="Incluir la sección de resumen (Git, Métricas) en el reporte."
    )
    parser.add_argument(
        "--dependencies",
        action="store_true",
        default=False,
        help="Incluir la sección detallada de dependencias en el reporte."
    )
    parser.add_argument(
        "--tasks",
        action="store_true",
        default=False,
        help="Incluir la lista agregada de tareas (TODOs/FIXMEs) en el reporte."
    )
    parser.add_argument(
        "--file-metadata",
        action="store_true",
        default=False,
        help="Incluir metadatos detallados (tamaño, hash) por archivo en el reporte."
    )
    # --- Fin Nuevos Flags ---

    args = parser.parse_args()

    try:
        configure_logging(args.log_level)
    except ValueError as e:
        print(f"Error de configuración: {e}")
        exit(1)

    logger.info("Iniciando RepoGPT con configuración: %s", args)

    # --- Selección de Extractores ---
    selected_extractor_names = [name.strip().lower() for name in args.extractors.split(',') if name.strip()]
    selected_extractor_instances: List[extractor_base.ExtractorModule] = []

    # Forzar la inclusión de extractores si sus secciones de reporte son requeridas,
    # incluso si el usuario no los especificó explícitamente en --extractors.
    required_extractors = set()
    if args.summary:
        required_extractors.add("git")
        required_extractors.add("metrics")
    if args.dependencies:
        required_extractors.add("dependencies")
    if args.tasks:
        required_extractors.add("todos")
    # 'todos' también es necesario para el conteo por archivo (comportamiento por defecto)
    required_extractors.add("todos")
    # 'dependencies' también es necesario para la lista simple (comportamiento por defecto)
    required_extractors.add("dependencies")


    final_extractor_names = set(selected_extractor_names) | required_extractors
    missing_required = required_extractors - set(AVAILABLE_EXTRACTORS.keys())
    if missing_required:
        logger.error("Se requieren los extractores %s para las opciones de reporte seleccionadas, pero no están disponibles.", missing_required)
        exit(1)

    for name in final_extractor_names:
        if name in AVAILABLE_EXTRACTORS:
            selected_extractor_instances.append(AVAILABLE_EXTRACTORS[name]())
            logger.debug("Extractor '%s' seleccionado.", name)
        elif name in selected_extractor_names: # Solo advertir si fue explícitamente pedido y no existe
             logger.warning("Extractor '%s' no reconocido. Ignorando.", name)

    if not selected_extractor_instances:
        logger.warning("No se seleccionó ningún extractor válido. El reporte puede estar vacío.")
        # No salimos, podríamos querer un reporte solo con la estructura base.
        # exit(1)

    # --- Selección de Reporter ---
    reporter_class = AVAILABLE_REPORTERS.get(args.format.lower())
    if not reporter_class:
        logger.error("Formato de reporte '%s' no válido. Abortando.", args.format)
        exit(1)
    reporter: reporter_base.Reporter = reporter_class()
    logger.debug("Usando reporter: %s", reporter.__class__.__name__)

    try:
        analyzer = RepositoryAnalyzer(
            repo_path=args.repo_path,
            start_path=args.start_path,
            max_workers=args.max_workers,
            max_file_size=args.max_file_size,
            use_gitignore=not args.no_gitignore,
        )

        # 1. Analizar la estructura y contenido de los archivos
        file_analysis_data = analyzer.analyze_repository()
        logger.info("Análisis de archivos completado.")

        # 2. Ejecutar extractores adicionales
        combined_data = {"files": file_analysis_data}
        for extractor in selected_extractor_instances:
             logger.debug("Ejecutando extractor: %s", extractor.__class__.__name__)
             try:
                 extracted_data = extractor.extract(analyzer, combined_data)
                 # Asegurarse de que la clave principal exista antes de actualizar
                 if extracted_data:
                      combined_data.update(extracted_data)
             except Exception as e:
                 extractor_name = extractor.__class__.__name__
                 logger.error("Error ejecutando extractor %s: %s", extractor_name, e, exc_info=True)
                 # Usar una clave de error más específica
                 if "errors" not in combined_data: combined_data["errors"] = {}
                 if "extractors" not in combined_data["errors"]: combined_data["errors"]["extractors"] = {}
                 combined_data["errors"]["extractors"][extractor_name] = str(e)


        logger.info("Extracción de información adicional completada.")

        # 3. Generar el reporte, pasando las opciones (args)
        # Verificar si el reporter acepta las opciones
        import inspect
        sig = inspect.signature(reporter.generate)
        if len(sig.parameters) > 1: # Si acepta más que 'self' y 'analysis_data'
            report_content = reporter.generate(combined_data, args)
        else:
             # Reporter antiguo o no soporta opciones (ej. JSONReporter)
             report_content = reporter.generate(combined_data)
             if args.summary or args.dependencies or args.tasks or args.file_metadata:
                   logger.warning("El reporter %s no soporta opciones de filtrado (--summary, etc.). Se incluirá toda la información.", reporter.__class__.__name__)

        logger.info("Generación de reporte completada.")

        # 4. Guardar o imprimir
        if args.output_file:
            try:
                args.output_file.parent.mkdir(parents=True, exist_ok=True)
                with args.output_file.open("w", encoding="utf-8") as f:
                    f.write(report_content)
                logger.info("Reporte guardado en: %s", args.output_file)
                print(f"Reporte guardado en: {args.output_file}")
            except IOError as e:
                logger.error("No se pudo escribir el archivo de salida %s: %s", args.output_file, e)
                # Imprimir si falla el guardado, pero ahora el reporte puede ser muy largo
                print(f"\n--- Reporte ({args.format}) ---")
                print("Error al guardar, imprimiendo las primeras 50 líneas:")
                print("\n".join(report_content.splitlines()[:50]))
                if len(report_content.splitlines()) > 50: print("[... truncado ...]")

        else:
            print(report_content)

    except (FileNotFoundError, NotADirectoryError, ValueError, RepoGPTException) as e: # Añadir RepoGPTException
         logger.error("Error de configuración o análisis: %s", e, exc_info=True)
         print(f"Error: {e}")
         exit(1)
    except Exception as e:
        logger.critical("Error inesperado durante la ejecución: %s", e, exc_info=True)
        print(f"Error inesperado: {e}")
        exit(1)

if __name__ == "__main__":
    main()
# ==================== FIN: ./__main__.py ======================

# ==================== INICIO: ./analyzer.py ====================
# repogpt/analyzer.py
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, Optional, Union
import os
import logging
from pathlib import Path

from .parsers.base import get_parser
from .utils.file_utils import calculate_file_hash, is_likely_binary
from .utils.gitignore_handler import get_gitignore_matcher, is_path_ignored


MAX_FILE_SIZE: int = 2 * 1024 * 1024  # 2MB

logger = logging.getLogger(__name__)

class RepositoryAnalyzer:
    def __init__(
        self,
        repo_path: Union[str, Path],
        start_path: str = "",
        max_depth: Optional[int] = None,
        max_workers: int = 4,
        max_file_size: int = MAX_FILE_SIZE,
        use_gitignore: bool = True
    ) -> None:
        self.repo_path = Path(repo_path).resolve()
        self.start_path = (self.repo_path / start_path).resolve()
        self.max_depth = max_depth
        self.max_workers = max_workers
        self.max_file_size = max_file_size
        
        self.gitignore_matcher = get_gitignore_matcher(self.repo_path) if use_gitignore else None

        self._validate_paths()
        logger.info("Analyzer inicializado para: %s", self.start_path)
    def _validate_paths(self) -> None:
        if not self.repo_path.is_dir():
            raise NotADirectoryError(f"Ruta del repositorio no es un directorio: {self.repo_path}")
        if not self.start_path.exists():
            raise FileNotFoundError(f"Ruta inicial no existe: {self.start_path}")
        # Asegurarse que start_path está dentro de repo_path
        try:
            self.start_path.relative_to(self.repo_path)
        except ValueError as e:
            raise ValueError(f"La ruta inicial {self.start_path} no está dentro del repositorio {self.repo_path}")


    def is_excluded(self, path: Path) -> bool:
        # 1. Verificar si está fuera del start_path (no debería pasar con os.walk bien usado)
        try:
            path.relative_to(self.start_path)
        except ValueError:
             logger.warning("Ruta %s fuera de la ruta de inicio %s", path, self.start_path)
             return True

        # 2. Excluir el directorio .git explícitamente
        if ".git" in path.relative_to(self.repo_path).parts:
             logger.debug("Excluido por ser parte de .git: %s", path)
             return True

        # 3. Gitignore (usando la librería)
        if self.gitignore_matcher and is_path_ignored(path, self.gitignore_matcher):
            return True# El log ya se hace dentro de is_path_ignored

        # 4. Detección de Binarios (si es archivo)
        if path.is_file() and is_likely_binary(path):
             logger.debug("Excluido por ser binario probable: %s", path)
             return True

        # 5. (Opcional) Añadir patrones de exclusión adicionales aquí si se reimplementan
        return False

    def _walk_directory(self):
        # Usar os.walk para manejar directorios y archivos de forma estándar
        for root, dirs, files in os.walk(self.start_path, topdown=True):
            current_path = Path(root)
            rel_depth = len(current_path.relative_to(self.start_path).parts)

            # --- Optimización: Filtrar directorios en una nueva lista ---
            valid_dirs = []
            excluded_dirs = [] # Para logging si es necesario
            for d in dirs:
                dir_path = current_path / d
                if self.is_excluded(dir_path):
                    excluded_dirs.append(d)
                    continue
                if self.max_depth is not None and rel_depth >= self.max_depth:
                    excluded_dirs.append(d)
                    logger.debug("Excluyendo directorio por profundidad: %s", dir_path)
                    continue
                valid_dirs.append(d)

            # Modificar dirs[:] al final
            dirs[:] = valid_dirs
            # Registrar directorios excluidos si se desea
            # if excluded_dirs:
            #    logger.debug("Directorios excluidos en %s: %s", current_path, excluded_dirs)
            # --- Fin Optimización ---


            # Rendimos los archivos del directorio actual que no están excluidos
            valid_files = []
            for f in files:
                file_path = current_path / f
                if not self.is_excluded(file_path):
                    valid_files.append(f)
                else:
                    # El log de exclusión ya se hace dentro de is_excluded o aquí si es necesario
                    # logger.debug("Excluyendo archivo: %s", file_path)
                    pass

            yield current_path, valid_dirs, valid_files # Yield los directorios válidos

    def analyze_repository(self) -> Dict[str, Any]:
        logger.info("Iniciando análisis en: %s", self.start_path)
        repo_analysis: Dict[str, Any] = {}

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {}
            # Iteramos sobre los directorios y archivos válidos
            for root_path, _, valid_files in self._walk_directory():
                for file_name in valid_files:
                    file_path = root_path / file_name
                    futures[executor.submit(self.process_file, file_path)] = file_path

            logger.info("Enviados %d archivos para procesamiento.", len(futures))

            processed_count = 0
            for future in as_completed(futures):
                file_path = futures[future]
                relative_path_str = str(file_path.relative_to(self.start_path)).replace('\\', '/')
                try:
                    result = future.result()
                    if result: # Si no es None (omitido por tamaño, etc.)
                        repo_analysis[relative_path_str] = result
                except Exception as e:
                    logger.error("Error procesando futuro para %s: %s", file_path, e, exc_info=True)
                    repo_analysis[relative_path_str] = {'error': f'Processing error: {e}'}
                processed_count += 1
                if processed_count % 100 == 0:
                     logger.info("Procesados %d/%d archivos...", processed_count, len(futures))

        logger.info("Análisis de archivos completado. Se procesaron %d archivos.", len(repo_analysis))
        return repo_analysis

    def process_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """Procesa un archivo individual."""
        logger.debug("Procesando: %s", file_path)
        try:
            stat = file_path.stat()
            file_size = stat.st_size
        except OSError as e:
            logger.error("No se pudo obtener stat para %s: %s", file_path, e)
            return {'error': f"OS Error getting stats: {e}"}

        if file_size > self.max_file_size:
            logger.warning("Archivo %s (%d bytes) excede el límite de %d bytes. Omitiendo.",
                           file_path, file_size, self.max_file_size)
            return None # Omitir archivo

        file_info: Dict[str, Any] = {
            'path': str(file_path), # Guardar path absoluto
            'size': file_size,
            'hash': calculate_file_hash(file_path)
        }

        parser = get_parser(file_path) # Obtener el parser adecuado 
        if parser:
            try:
                parsed_data = parser.parse(file_path, file_info)
                file_info.update(parsed_data)
            except Exception as e:
                logger.error("Error durante el parsing de %s con %s: %s",
                               file_path, parser.__class__.__name__, e, exc_info=True)
                file_info['error'] = f"Parsing error ({parser.__class__.__name__}): {e}"
        else:
             logger.debug("No se encontró parser específico para: %s. Aplicando análisis genérico.", file_path)

        return file_info
# ==================== FIN: ./analyzer.py ======================

# ==================== INICIO: ./exceptions.py ====================
# repogpt/exceptions.py

class RepoGPTException(Exception):
    """Clase base para excepciones específicas de RepoGPT."""
    pass

class ConfigurationError(RepoGPTException):
    """Error relacionado con la configuración del análisis."""
    pass

class ParsingError(RepoGPTException):
    """Error durante la fase de parsing de un archivo."""
    def __init__(self, message: str, file_path: str = None, parser_name: str = None):
        self.file_path = file_path
        self.parser_name = parser_name
        detail = f"Parser: {parser_name or 'N/A'}, File: {file_path or 'N/A'}"
        full_message = f"{message} ({detail})"
        super().__init__(full_message)

class AnalysisError(RepoGPTException):
    """Error durante la fase principal de análisis del repositorio."""
    pass

class ReportingError(RepoGPTException):
    """Error durante la generación del reporte."""
    pass
# ==================== FIN: ./exceptions.py ======================

# ==================== INICIO: ./utils/logging.py ====================
# repogpt/utils/logging.py
import logging
import sys

def configure_logging(log_level: str) -> None:
    levels = {
        'DEBUG': logging.DEBUG,
        'INFO': logging.INFO,
        'WARNING': logging.WARNING,
        'ERROR': logging.ERROR,
        'CRITICAL': logging.CRITICAL
    }
    level = levels.get(log_level.upper())
    if level is None:
        raise ValueError(f"Nivel de log inválido: {log_level}. Usar: {', '.join(levels.keys())}")

    # Configuración básica (podría ser más avanzada)
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler("repogpt_analyzer.log"), # Renombrado
            logging.StreamHandler(sys.stdout) # Usar stdout
        ]
    )
    # Silenciar logs muy verbosos de otras librerías si es necesario
    # logging.getLogger("some_library").setLevel(logging.WARNING)
# ==================== FIN: ./utils/logging.py ======================

# ==================== INICIO: ./utils/__init__.py ====================
# repogpt/utils/__init__.py

# Opcional: re-exportar funciones útiles
# from .logging import configure_logging
# from .file_utils import calculate_file_hash, is_likely_binary
# from .gitignore import load_gitignore_matcher # Si se usa la versión con librería
# ==================== FIN: ./utils/__init__.py ======================

# ==================== INICIO: ./utils/file_utils.py ====================
# repogpt/utils/file_utils.py

import hashlib
import logging
from pathlib import Path
from typing import Optional

logger = logging.getLogger(__name__)

CHUNK_SIZE = 8192  # Leer archivos en bloques de 8KB para hashing

def calculate_file_hash(file_path: Path, algorithm: str = 'sha256') -> Optional[str]:
    """
    Calcula el hash de un archivo usando el algoritmo especificado.

    Args:
        file_path: Ruta al archivo.
        algorithm: Algoritmo de hash a usar (ej. 'sha256', 'md5').

    Returns:
        El hash en formato hexadecimal como string, o None si ocurre un error.
    """
    try:
        hasher = hashlib.new(algorithm)
        with file_path.open('rb') as f:
            while chunk := f.read(CHUNK_SIZE):
                hasher.update(chunk)
        return hasher.hexdigest()
    except FileNotFoundError:
        logger.error("Archivo no encontrado al calcular hash: %s", file_path)
    except PermissionError:
        logger.error("Permiso denegado al calcular hash para: %s", file_path)
    except OSError as e:
        logger.error("Error de OS calculando hash para %s: %s", file_path, e)
    except ValueError:
        logger.error("Algoritmo de hash inválido: %s", algorithm)
    except Exception as e:
        logger.error("Error inesperado calculando hash para %s: %s", file_path, e, exc_info=True)

    return None

def is_likely_binary(file_path: Path, check_bytes: int = 1024) -> bool:
    """
    Intenta determinar si un archivo es probablemente binario.

    Actualmente usa una heurística simple: la presencia de un byte NULL
    dentro de los primeros 'check_bytes'.

    Args:
        file_path: Ruta al archivo.
        check_bytes: Número de bytes iniciales a revisar.

    Returns:
        True si el archivo parece binario, False en caso contrario o si hay error.
    """
    try:
        with file_path.open('rb') as f:
            chunk = f.read(check_bytes)
            if b'\x00' in chunk:
                logger.debug("Detectado byte NULL en %s, marcando como binario.", file_path)
                return True
            # Podríamos añadir más heurísticas aquí si fuera necesario
            # Por ejemplo, buscar un alto porcentaje de caracteres no imprimibles.
    except FileNotFoundError:
        logger.warning("Archivo no encontrado al verificar si es binario: %s", file_path)
        return False # No se puede determinar, asumir no binario por seguridad? O True? Depende del caso de uso. Falso es más seguro para no excluir texto.
    except PermissionError:
        logger.warning("Permiso denegado al verificar si es binario: %s", file_path)
        return False # Asumir no binario
    except OSError as e:
        logger.warning("Error de OS verificando si %s es binario: %s", file_path, e)
        return False # Asumir no binario
    except Exception as e:
        logger.warning("Error inesperado verificando si %s es binario: %s", file_path, e)
        return False # Asumir no binario

    logger.debug("%s no parece binario (basado en byte NULL).", file_path)
    return False

# Podrías añadir aquí otras utilidades relacionadas con archivos si las necesitas.
# ==================== FIN: ./utils/file_utils.py ======================

# ==================== INICIO: ./utils/gitignore_handler.py ====================
# repogpt/utils/gitignore.py
import logging
from pathlib import Path
from typing import Callable, Optional,Dict

# Importar la librería
try:
    import gitignore_parser
    gitignore_parser_available = True
except ImportError:
    gitignore_parser = None
    gitignore_parser_available = False
    logging.warning("gitignore-parser no instalado. El manejo de .gitignore será limitado o inexistente.")
    logging.warning("Instala con: pip install gitignore-parser")

logger = logging.getLogger(__name__)

# Variable global para cachear el matcher (opcional, pero puede mejorar rendimiento)
_cached_matchers: Dict[Path, Optional[Callable[[str], bool]]] = {}

def _load_matcher_uncached(gitignore_path: Path) -> Optional[Callable[[str], bool]]:
    """Carga el matcher desde un archivo .gitignore sin usar caché."""
    if not gitignore_path.is_file():
        logger.debug(".gitignore no encontrado en %s", gitignore_path)
        return None

    if not gitignore_parser_available:
        logger.warning("gitignore-parser no está disponible, no se puede parsear %s", gitignore_path)
        return None # No podemos hacer matching

    try:
        with gitignore_path.open('r', encoding='utf-8', errors='ignore') as f:
            # gitignore_parser.parse devuelve una función que toma un path y devuelve True si coincide
            matcher = gitignore_parser.parse(f)
            logger.info("Matcher de .gitignore cargado desde %s", gitignore_path)
            return matcher
    except Exception as e:
        logger.error("Error parseando .gitignore en %s: %s", gitignore_path, e, exc_info=True)
        return None # Error al parsear, tratar como si no hubiera gitignore

def get_gitignore_matcher(repo_path: Path, use_cache: bool = True) -> Optional[Callable[[str], bool]]:
    """
    Obtiene la función matcher para el .gitignore del repositorio, opcionalmente desde caché.

    Args:
        repo_path: La ruta raíz del repositorio donde buscar .gitignore.
        use_cache: Si se debe usar la caché de matchers.

    Returns:
        Una función que toma un path (string) y devuelve True si debe ser ignorado,
        o None si no hay .gitignore o no se pudo parsear.
    """
    gitignore_path = repo_path / ".gitignore"

    if use_cache:
        if gitignore_path in _cached_matchers:
            logger.debug("Usando matcher de .gitignore cacheado para %s", gitignore_path)
            return _cached_matchers[gitignore_path]
        else:
            matcher = _load_matcher_uncached(gitignore_path)
            _cached_matchers[gitignore_path] = matcher
            return matcher
    else:
        # Forzar recarga si no se usa caché
        return _load_matcher_uncached(gitignore_path)

def is_path_ignored(absolute_path: Path, matcher: Optional[Callable[[str], bool]]) -> bool:
    """
    Verifica si una ruta absoluta debe ser ignorada según el matcher de .gitignore.

    Args:
        absolute_path: La ruta absoluta del archivo o directorio a verificar.
        matcher: La función matcher obtenida de get_gitignore_matcher.

    Returns:
        True si la ruta debe ser ignorada, False en caso contrario.
    """
    if matcher is None:
        return False # No hay matcher, no ignorar

    try:
        # La librería espera un path string absoluto
        is_ignored = matcher(str(absolute_path))
        if is_ignored:
             logger.debug("Ruta %s ignorada por .gitignore.", absolute_path)
        return is_ignored
    except Exception as e:
         # Captura errores inesperados del matcher
         logger.error("Error aplicando matcher de gitignore a %s: %s", absolute_path, e, exc_info=True)
         return False # Mejor no ignorar si hay error en el matcher

# --- Funciones antiguas (load_gitignore_patterns, is_path_ignored con fnmatch) ---
# --- Se pueden eliminar o mantener comentadas como referencia ---
# def load_gitignore_patterns(repo_path: Path) -> List[str]: ...
# def is_path_ignored_fnmatch(...) -> bool: ...
# ==================== FIN: ./utils/gitignore_handler.py ======================

# ==================== INICIO: ./utils/text_processing.py ====================
# repogpt/utils/text_processing.py
import re
import logging
from typing import List, Dict, Any, Tuple

logger = logging.getLogger(__name__)

# Patrón general para TODO/FIXME, busca la palabra clave seguida opcionalmente
# por dos puntos o espacio, y captura el resto del mensaje en esa línea/bloque.
TODO_FIXME_PATTERN = re.compile(r'(TODO|FIXME)[\s:]?(.*)', re.IGNORECASE)

def extract_todos_fixmes_from_comments(
    comments: List[Dict[str, Any]]
) -> Dict[str, List[Dict[str, Any]]]:
    """
    Extrae TODOs y FIXMEs encontrados dentro de una lista de comentarios pre-extraídos.

    Args:
        comments: Una lista de diccionarios, donde cada dict representa un comentario
                  y debe tener al menos las claves 'line' (int o str) y 'text' (str).

    Returns:
        Un diccionario con claves 'todos' y 'fixmes', cada una conteniendo una lista
        de diccionarios {'line': ..., 'message': ...}.
    """
    todos: List[Dict[str, Any]] = []
    fixmes: List[Dict[str, Any]] = []

    for comment in comments:
        comment_text = comment.get('text', '')
        comment_line = comment.get('line', 'unknown') # Mantener 'block' o 'comment' si no hay línea específica

        # Buscar todas las ocurrencias en el texto del comentario
        for match in TODO_FIXME_PATTERN.finditer(comment_text):
            marker = match.group(1).upper()
            message = match.group(2).strip()
            # Eliminar saltos de línea del mensaje si es un comentario multilínea
            message = message.replace('\n', ' ').replace('\r', '')

            entry = {'line': comment_line, 'message': message}
            if marker == 'TODO':
                todos.append(entry)
            else: # FIXME
                fixmes.append(entry)

    return {'todos': todos, 'fixmes': fixmes}

def extract_comments_from_content(
    content: str,
    line_comment_patterns: List[str] = [], # Regex para comentarios de línea (ej. r'^\s*#\s*(.*)')
    block_comment_patterns: List[Tuple[str, str]] = [], # Tuplas de regex (inicio, fin) para bloques (ej. (r'/\*', r'\*/'))
    include_docstrings: bool = False # Placeholder si se quiere añadir extracción genérica de docstrings
) -> List[Dict[str, Any]]:
    """
    Extrae comentarios de un bloque de contenido usando patrones Regex.

    Args:
        content: El contenido completo del archivo como string.
        line_comment_patterns: Lista de patrones regex para comentarios de una línea.
                               El grupo 1 debe capturar el texto del comentario.
        block_comment_patterns: Lista de tuplas (start_regex, end_regex) para comentarios de bloque.
                                La extracción será básica (extrae todo entre start y end).
        include_docstrings: (No implementado aún) Si intentar extraer docstrings genéricos.


    Returns:
        Lista de diccionarios de comentarios {'line': int, 'text': str} o {'line': 'block', 'text': str}.
        El número de línea es aproximado para bloques.
    """
    comments: List[Dict[str, Any]] = []
    lines = content.splitlines()

    # --- Comentarios de Línea ---
    compiled_line_patterns = [re.compile(p) for p in line_comment_patterns]
    for i, line in enumerate(lines):
        for pattern in compiled_line_patterns:
            match = pattern.search(line) # Usar search para encontrar en cualquier parte, no solo al inicio
            if match:
                # Intentar capturar grupo 1, si no, tomar todo después del inicio del comentario
                try:
                    comment_text = match.group(1).strip()
                except IndexError:
                    # Heurística: encontrar dónde empezó el match y tomar el resto
                    start_index = match.start()
                    # Podríamos buscar el delimitador real (ej. '#', '//')
                    comment_text = line[start_index:].lstrip('#/ \t').strip() # Simple fallback

                comments.append({'line': i + 1, 'text': comment_text})
                # Podríamos añadir 'break' si un tipo de comentario excluye otros en la misma línea

    # --- Comentarios de Bloque (Básico) ---
    # Nota: Esto es muy simplificado y no maneja anidamiento ni comentarios dentro de strings.
    # Para lenguajes complejos (JS, Python), es mejor usar tokenizers/parsers específicos.
    current_pos = 0
    for start_pattern_str, end_pattern_str in block_comment_patterns:
        try:
            # Usar re.DOTALL para que '.' coincida con saltos de línea
            block_regex = re.compile(re.escape(start_pattern_str) + r'(.*?)' + re.escape(end_pattern_str), re.DOTALL)
            for match in block_regex.finditer(content):
                 # Calcular línea aproximada (podría ser inexacto)
                 line_approx = content.count('\n', 0, match.start()) + 1
                 text = match.group(1).strip()
                 comments.append({'line': f'block~{line_approx}', 'text': text})
        except Exception as e:
             logger.warning("Error compilando/ejecutando regex de comentario de bloque (%s, %s): %s",
                            start_pattern_str, end_pattern_str, e)

    # Ordenar por línea (aproximada para bloques)
    def sort_key(comment):
        line = comment['line']
        if isinstance(line, str) and line.startswith('block~'):
            try: return int(line.split('~')[1])
            except: return float('inf') # Poner bloques sin línea al final
        elif isinstance(line, int):
            return line
        return float('inf') # Poner 'unknown' al final

    comments.sort(key=sort_key)
    return comments

# --- Función para contar líneas en blanco ---
def count_blank_lines(content: str) -> int:
    """Cuenta las líneas que están vacías o contienen solo espacios en blanco."""
    count = 0
    for line in content.splitlines():
        if not line.strip():
            count += 1
    return count
# ==================== FIN: ./utils/text_processing.py ======================

# ==================== INICIO: ./extractors/__init__.py ====================
# repogpt/extractors/__init__.py

# Importar los módulos específicos para que se registren o sean accesibles
from . import dependencies
from . import git
from . import metrics
from . import structure
from . import todos

# Re-exportar la clase base para conveniencia
from .base import ExtractorModule

__all__ = [
    'ExtractorModule',
    'dependencies',
    'git',
    'metrics',
    'structure',
    'todos',
]
# ==================== FIN: ./extractors/__init__.py ======================

# ==================== INICIO: ./extractors/base.py ====================
# repogpt/extractors/base.py

import abc
import logging
from typing import Any, Dict

# Importar la clase principal para type hints si es necesario (evitando importación circular)
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from repogpt.analyzer import RepositoryAnalyzer

logger = logging.getLogger(__name__)

class ExtractorModule(abc.ABC):
    """Clase base abstracta para módulos que extraen información específica."""

    @abc.abstractmethod
    def extract(self, analyzer: 'RepositoryAnalyzer', analyzed_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extrae información específica del repositorio o de los datos ya analizados.

        Args:
            analyzer: La instancia de RepositoryAnalyzer (para acceso a repo_path, etc.).
            analyzed_data: El diccionario que contiene los resultados del análisis
                           de archivos realizado por RepositoryAnalyzer (bajo la clave 'files').
                           Este diccionario puede ser modificado por extractores anteriores.

        Returns:
            Un diccionario que contiene la información extraída. La clave principal
            debe ser única para este extractor (ej. 'dependencies', 'git_info').
            Este diccionario se fusionará con los resultados generales.
            Debe devolver un diccionario vacío ({}) si no se extrae nada.
        """
        pass
# ==================== FIN: ./extractors/base.py ======================

# ==================== INICIO: ./extractors/dependencies.py ====================
# repogpt/extractors/dependencies.py

import json
import logging
import re
from pathlib import Path
from typing import Any, Dict, List, Optional

from .base import ExtractorModule
from repogpt.analyzer import RepositoryAnalyzer # Importación directa ahora es segura

try:
    import yaml
except ImportError:
    yaml = None
    logging.debug("PyYAML no instalado, no se analizarán archivos .yaml para dependencias.")

try:
    import toml
except ImportError:
    toml = None
    logging.debug("toml library no instalado, no se analizarán archivos pyproject.toml.")


logger = logging.getLogger(__name__)

class DependencyExtractor(ExtractorModule):
    """Extrae información de dependencias de archivos comunes."""

    # Mapeo de nombres de archivo a métodos de parseo
    dependency_parsers = {
        'package.json': '_parse_package_json',
        'requirements.txt': '_parse_requirements_txt',
        'Pipfile': '_parse_pipfile',
        'pyproject.toml': '_parse_pyproject',
        # 'poetry.lock': '_parse_poetry_lock', # Lock files can be very large, maybe skip?
        'environment.yaml': '_parse_environment_yaml', # Common for Conda
        'environment.yml': '_parse_environment_yaml',
        # Añadir más si es necesario: Gemfile, go.mod, pom.xml, build.gradle, etc.
    }

    def extract(self, analyzer: RepositoryAnalyzer, analyzed_data: Dict[str, Any]) -> Dict[str, Any]:
        """Busca y parsea archivos de dependencias conocidos en la raíz del repo."""
        logger.info("Buscando archivos de dependencias...")
        deps: Dict[str, Any] = {}
        repo_root = analyzer.repo_path # Usar el path base del analyzer

        for file_name, parser_method_name in self.dependency_parsers.items():
            file_path = repo_root / file_name
            if file_path.exists():
                logger.debug("Encontrado archivo de dependencias: %s", file_path)
                parser_method = getattr(self, parser_method_name, None)
                if parser_method:
                    try:
                        deps[file_name] = parser_method(file_path)
                        logger.info("Dependencias extraídas de: %s", file_name)
                    except Exception as e:
                        logger.error("Error procesando archivo de dependencias %s: %s", file_name, e, exc_info=True)
                        deps[file_name] = {'_error': f'Failed to parse: {e}'}
                else:
                     logger.warning("Método parser '%s' no encontrado en DependencyExtractor.", parser_method_name)
            else:
                logger.debug("Archivo de dependencias no encontrado: %s", file_path)

        return {"dependencies": deps} # Devuelve bajo la clave 'dependencies'

    # --- Métodos de Parseo Específicos ---

    def _parse_package_json(self, path: Path) -> Dict[str, Any]:
        logger.debug("Parseando %s", path.name)
        try:
            with path.open('r', encoding='utf-8') as f:
                data = json.load(f)
            # Extraer solo las secciones relevantes
            return {
                'name': data.get('name'),
                'version': data.get('version'),
                'dependencies': data.get('dependencies', {}),
                'devDependencies': data.get('devDependencies', {}),
                'peerDependencies': data.get('peerDependencies', {}),
                'optionalDependencies': data.get('optionalDependencies', {}),
            }
        except json.JSONDecodeError as e:
            logger.error("Error de JSON en %s: %s", path.name, e)
            return {'_error': f'Invalid JSON: {e}'}

    def _parse_requirements_txt(self, path: Path) -> List[str]:
        logger.debug("Parseando %s", path.name)
        lines: List[str] = []
        try:
            with path.open('r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    # Omitir comentarios y líneas vacías, manejar continuaciones básicas
                    line = re.sub(r'\s+#.*$', '', line) # Quitar comentarios en línea
                    if line and not line.startswith('#'):
                        # Simple manejo de continuación de línea (puede no ser perfecto)
                        while line.endswith('\\'):
                            line = line[:-1].strip() + next(f, '').strip()
                        lines.append(line)
            return lines
        except Exception as e:
            logger.error("Error leyendo %s: %s", path.name, e)
            return [{'_error': f'Error reading file: {e}'}] # Devuelve error como parte de la lista? O solo log?

    def _parse_pipfile(self, path: Path) -> Dict[str, Any]:
        logger.debug("Parseando %s", path.name)
        if toml is None:
            return {'_error': 'toml library not installed'}
        try:
            data = toml.load(path)
            return {
                'packages': data.get('packages', {}),
                'dev-packages': data.get('dev-packages', {})
            }
        except toml.TomlDecodeError as e:
            logger.error("Error de TOML en %s: %s", path.name, e)
            return {'_error': f'Invalid TOML: {e}'}

    def _parse_pyproject(self, path: Path) -> Dict[str, Any]:
        logger.debug("Parseando %s", path.name)
        if toml is None:
            return {'_error': 'toml library not installed'}
        try:
            data = toml.load(path)
            # Buscar dependencias en varias secciones comunes (Poetry, PDM, build-system)
            dependencies = {}
            if poetry_deps := data.get('tool', {}).get('poetry', {}).get('dependencies'):
                dependencies['poetry'] = poetry_deps
            if pdm_deps := data.get('project', {}).get('dependencies'):
                 dependencies['pdm/build'] = pdm_deps # PEP 621
            if build_reqs := data.get('build-system', {}).get('requires'):
                 dependencies['build-system'] = build_reqs
            return dependencies
        except toml.TomlDecodeError as e:
            logger.error("Error de TOML en %s: %s", path.name, e)
            return {'_error': f'Invalid TOML: {e}'}

    def _parse_environment_yaml(self, path: Path) -> Dict[str, Any]:
        logger.debug("Parseando %s", path.name)
        if yaml is None:
            return {'_error': 'PyYAML library not installed'}
        try:
            with path.open('r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
            return {
                'name': data.get('name'),
                'channels': data.get('channels', []),
                'dependencies': data.get('dependencies', [])
            }
        except yaml.YAMLError as e:
            logger.error("Error de YAML en %s: %s", path.name, e)
            return {'_error': f'Invalid YAML: {e}'}
# ==================== FIN: ./extractors/dependencies.py ======================

# ==================== INICIO: ./extractors/git.py ====================
# repogpt/extractors/git.py
import logging
import subprocess
from pathlib import Path
from typing import Any, Dict

from .base import ExtractorModule
from repogpt.analyzer import RepositoryAnalyzer

logger = logging.getLogger(__name__)

class GitInfoExtractor(ExtractorModule):
    """Extrae información del repositorio Git si existe."""

    def extract(self, analyzer: RepositoryAnalyzer, analyzed_data: Dict[str, Any]) -> Dict[str, Any]:
        """Ejecuta comandos git para obtener información del último commit, rama, etc."""
        repo_path = analyzer.repo_path
        git_dir = repo_path / ".git"

        if not git_dir.exists():
            logger.info("Directorio .git no encontrado en %s. Omitiendo extracción Git.", repo_path)
            return {}

        logger.info("Extrayendo información de Git desde: %s", repo_path)
        git_info: Dict[str, Any] = {}
        repo_str = str(repo_path) # Para usar con -C

        # Comandos a ejecutar y la clave bajo la cual guardar el resultado
        commands = {
            'commit_hash': ['rev-parse', 'HEAD'],
            'commit_short_hash': ['rev-parse', '--short', 'HEAD'],
            'commit_message': ['log', '-1', '--pretty=%B'],
            'author_name': ['log', '-1', '--pretty=%an'],
            'author_email': ['log', '-1', '--pretty=%ae'],
            'commit_date': ['log', '-1', '--pretty=%cI'], # ISO 8601
            'branch': ['rev-parse', '--abbrev-ref', 'HEAD'],
            'tags': ['tag', '--points-at', 'HEAD'],
            'remotes': ['remote', '-v'],
            # 'last_commit_stats': ['show', '--stat', '--oneline', 'HEAD'] # Puede ser muy largo
        }

        for key, cmd in commands.items():
            try:
                # Usar stderr=subprocess.PIPE para capturar errores de git
                process = subprocess.run(
                    ['git', '-C', repo_str] + cmd,
                    capture_output=True,
                    text=True, # Decodificar salida como texto
                    check=False, # No lanzar excepción si git falla, lo manejamos abajo
                    encoding='utf-8', # Especificar encoding
                    errors='replace'  # Manejar errores de decodificación
                )
                if process.returncode == 0:
                    output = process.stdout.strip()
                    # Procesar salida específica si es necesario
                    if key == 'remotes':
                        remotes_list = []
                        for line in output.splitlines():
                             parts = line.split()
                             if len(parts) >= 2:
                                 remotes_list.append({'name': parts[0], 'url': parts[1]})
                        git_info[key] = remotes_list
                    elif key == 'tags':
                         git_info[key] = output.splitlines()
                    else:
                         git_info[key] = output
                    logger.debug("Git %s: %s", key, git_info[key])
                else:
                    error_msg = process.stderr.strip()
                    logger.warning("Comando git falló para '%s': %s (código: %d)",
                                   key, error_msg or "Sin salida de error", process.returncode)
                    git_info[key] = f"_error: Git command failed (code {process.returncode})"

            except FileNotFoundError:
                 logger.error("Comando 'git' no encontrado. Asegúrate de que Git esté instalado y en el PATH.")
                 git_info['_error_git_not_found'] = "Git command not found."
                 break # No intentar más comandos si git no existe
            except Exception as e:
                logger.error("Error inesperado ejecutando comando git para '%s': %s", key, e, exc_info=True)
                git_info[key] = f"_error: Unexpected error ({e})"

        return {"git_info": git_info}
# ==================== FIN: ./extractors/git.py ======================

# ==================== INICIO: ./extractors/metrics.py ====================
# repogpt/extractors/metrics.py
import logging
from pathlib import Path
from typing import Any, Dict

from .base import ExtractorModule
from repogpt.analyzer import RepositoryAnalyzer

logger = logging.getLogger(__name__)

class CodeMetricsExtractor(ExtractorModule):
    """Calcula métricas básicas del código analizado."""

    def extract(self, analyzer: RepositoryAnalyzer, analyzed_data: Dict[str, Any]) -> Dict[str, Any]:
        """Calcula totales y estadísticas por tipo de archivo."""
        logger.info("Calculando métricas de código...")
        metrics: Dict[str, Any] = {
            "total_files": 0,
            "total_lines": 0,
            "total_size_bytes": 0,
            "total_comment_lines": 0, # Renombrado para claridad
            "total_blank_lines": 0,
            "code_lines": 0, # Calculado al final
            "files_by_extension": {},
            "files_with_errors": 0,
            "files_missing_line_data": 0, # Nuevo contador
        }

        files_data = analyzed_data.get("files", {})
        if not files_data:
            logger.warning("No hay datos de archivos ('files') para calcular métricas.")
            return {"code_metrics": metrics}

        for relative_path, file_info in files_data.items():
            if not isinstance(file_info, dict):
                logger.warning("Entrada inesperada en 'files' para '%s', omitiendo métricas.", relative_path)
                continue

            metrics["total_files"] += 1
            metrics["total_size_bytes"] += file_info.get('size', 0)

            # Usar la extensión del path relativo
            ext = Path(relative_path).suffix.lower()
            if not ext:
                ext = "_no_extension_"
            metrics["files_by_extension"][ext] = metrics["files_by_extension"].get(ext, 0) + 1

            if '_error' in file_info:
                metrics["files_with_errors"] += 1
                # No sumar líneas si hubo error, pero verificar si tenemos conteo parcial
                if 'line_count' not in file_info:
                     metrics["files_missing_line_data"] += 1
                # Aún así, sumar el tamaño y contar el archivo por extensión

            # Sumar líneas solo si no hubo error grave y tenemos el conteo
            elif 'line_count' in file_info:
                line_count = file_info['line_count']
                metrics["total_lines"] += line_count

                # Sumar comentarios y líneas en blanco si están disponibles
                comment_count = file_info.get('comments_count', 0)
                blank_count = file_info.get('blank_lines', 0) # Obtener líneas en blanco
                metrics["total_comment_lines"] += comment_count
                metrics["total_blank_lines"] += blank_count

            else:
                # El archivo se procesó pero el parser no devolvió 'line_count'
                 metrics["files_missing_line_data"] += 1


        # Calcular líneas de código al final
        metrics["code_lines"] = (metrics["total_lines"] -
                                 metrics["total_comment_lines"] -
                                 metrics["total_blank_lines"])
        # Asegurarse de que no sea negativo si los contadores son inconsistentes
        metrics["code_lines"] = max(0, metrics["code_lines"])


        log_msg = (f"Métricas calculadas: {metrics['total_files']} archivos, "
                   f"{metrics['total_lines']} líneas totales, "
                   f"{metrics['total_comment_lines']} coment., "
                   f"{metrics['total_blank_lines']} en blanco, "
                   f"~{metrics['code_lines']} código.")
        if metrics['files_missing_line_data'] > 0:
             log_msg += f" ({metrics['files_missing_line_data']} archivos sin datos de líneas)"
        if metrics['files_with_errors'] > 0:
             log_msg += f" ({metrics['files_with_errors']} con errores)"

        logger.info(log_msg)
        return {"code_metrics": metrics}
# ==================== FIN: ./extractors/metrics.py ======================

# ==================== INICIO: ./extractors/structure.py ====================
# repogpt/extractors/structure.py
import logging
from pathlib import Path
from typing import Any, Dict, List

from .base import ExtractorModule
from repogpt.analyzer import RepositoryAnalyzer

logger = logging.getLogger(__name__)

class StructureExtractor(ExtractorModule):
    """Extrae una visión general de la estructura (clases, funciones) por archivo."""

    def extract(self, analyzer: RepositoryAnalyzer, analyzed_data: Dict[str, Any]) -> Dict[str, Any]:
        """Itera sobre los archivos analizados y extrae elementos estructurales clave."""
        logger.info("Extrayendo resumen de estructura...")
        structure_summary: Dict[str, Dict[str, List[str]]] = {}

        files_data = analyzed_data.get("files", {})
        if not files_data:
            logger.warning("No hay datos de archivos ('files') para extraer estructura.")
            return {"structure_summary": structure_summary}

        for relative_path, file_info in files_data.items():
            if not isinstance(file_info, dict) or '_error' in file_info:
                continue 

            file_structure: Dict[str, List[str]] = {}

            # Extraer nombres de clases
            if classes := file_info.get('classes'):
                if isinstance(classes, list):
                    # Asumiendo que cada item de 'classes' es un dict con 'name'
                    file_structure['classes'] = [c.get('name', 'N/A') for c in classes if isinstance(c, dict)]

            # Extraer nombres de funciones (a nivel de módulo)
            if functions := file_info.get('functions'):
                 if isinstance(functions, list):
                    # Asumiendo que cada item de 'functions' es un dict con 'name'
                     file_structure['functions'] = [f.get('name', 'N/A') for f in functions if isinstance(f, dict)]

            # Extraer nombres de componentes (de JS/TS)
            if components := file_info.get('components'):
                 if isinstance(components, list):
                    file_structure['components'] = [str(comp) for comp in components] # Asegurar que sean strings

            # Extraer exports (de JS/TS)
            if exports := file_info.get('exports'):
                 if isinstance(exports, list):
                    file_structure['exports'] = [str(exp) for exp in exports]

            # Añadir otros elementos si los parsers los proporcionan (ej. 'interfaces' de TS)

            if file_structure: # Solo añadir si se encontró algo
                 structure_summary[relative_path] = file_structure
                 logger.debug("Estructura extraída para: %s", relative_path)

        logger.info("Resumen de estructura generado para %d archivos.", len(structure_summary))
        return {"structure_summary": structure_summary}
# ==================== FIN: ./extractors/structure.py ======================

# ==================== INICIO: ./extractors/todos.py ====================
# repogpt/extractors/todos.py
import logging
from typing import Any, Dict, List

from .base import ExtractorModule
from repogpt.analyzer import RepositoryAnalyzer

logger = logging.getLogger(__name__)

class TodoFixmeExtractor(ExtractorModule):
    """Agrega todos los TODOs y FIXMEs encontrados por los parsers."""

    def extract(self, analyzer: RepositoryAnalyzer, analyzed_data: Dict[str, Any]) -> Dict[str, Any]:
        """Recopila las entradas 'todos_fixmes' de cada archivo analizado."""
        logger.info("Agregando TODOs y FIXMEs...")
        aggregated_todos: Dict[str, List[Dict[str, Any]]] = {}
        aggregated_fixmes: Dict[str, List[Dict[str, Any]]] = {}
        total_todos = 0
        total_fixmes = 0

        files_data = analyzed_data.get("files", {})
        if not files_data:
            logger.warning("No hay datos de archivos ('files') para agregar TODOs/FIXMEs.")
            return {"aggregated_tasks": {"todos": {}, "fixmes": {}}}

        for relative_path, file_info in files_data.items():
            if not isinstance(file_info, dict) or '_error' in file_info:
                continue

            if todos_fixmes := file_info.get('todos_fixmes'):
                if isinstance(todos_fixmes, dict):
                    if todos_list := todos_fixmes.get('todos'):
                        if isinstance(todos_list, list) and todos_list:
                            aggregated_todos[relative_path] = todos_list
                            total_todos += len(todos_list)
                    if fixmes_list := todos_fixmes.get('fixmes'):
                         if isinstance(fixmes_list, list) and fixmes_list:
                            aggregated_fixmes[relative_path] = fixmes_list
                            total_fixmes += len(fixmes_list)

        logger.info("Agregación completada: %d TODOs, %d FIXMEs encontrados.", total_todos, total_fixmes)
        return {
            "aggregated_tasks": {
                "todos": aggregated_todos,
                "fixmes": aggregated_fixmes,
                "total_todos": total_todos,
                "total_fixmes": total_fixmes
            }
        }
# ==================== FIN: ./extractors/todos.py ======================

# ==================== INICIO: ./parsers/__init__.py ====================
# repogpt/parsers/__init__.py
import logging
from . import python
from . import markdown
from . import javascript
from . import yaml_parser
from . import html
# from . import dockerfile_
from . import generic # Importar el genérico/fallback si existe


# Re-exportar la función principal para obtener parsers
from .base import get_parser

__all__ = ['get_parser']
# ==================== FIN: ./parsers/__init__.py ======================

# ==================== INICIO: ./parsers/base.py ====================
# repogpt/parsers/base.py

import abc
import logging
from pathlib import Path
from typing import Any, Dict, Optional, Type

# *** NO IMPORTAR dockerfile NI generic AQUÍ ARRIBA ***
from ..utils.file_utils import is_likely_binary

logger = logging.getLogger(__name__)


# --- Interfaz Base del Parser ---
class Parser(abc.ABC):
    """Clase base abstracta para todos los parsers de archivos."""

    @abc.abstractmethod
    def parse(self, file_path: Path, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analiza el contenido del archivo dado.

        Args:
            file_path: Ruta al archivo a analizar.
            file_info: Diccionario con metadatos pre-calculados (size, hash, etc.).
                       El parser puede añadir o modificar este diccionario.

        Returns:
            Un diccionario con la información extraída específica del tipo de archivo.
            Este diccionario será fusionado con file_info.
            Debe devolver un diccionario vacío ({}) si no hay nada específico que extraer.
            Puede lanzar excepciones si el parsing falla catastróficamente,
            aunque es preferible devolver {'error': 'mensaje'} si es posible.
        """
        pass

# --- Registro de Parsers ---
# Mapea extensiones de archivo (en minúsculas) a la clase Parser correspondiente.
_parser_registry: Dict[str, Type[Parser]] = {}
def register_parser(extension: str, parser_class: Type[Parser]) -> None:
    """Registra una clase Parser para una extensión de archivo específica."""
    if not extension.startswith('.'):
        logger.warning("La extensión '%s' no empieza con '.', podría causar problemas.", extension)

    ext_lower = extension.lower()
    if ext_lower in _parser_registry:
        logger.warning("Extensión '%s' ya registrada con %s. Sobrescribiendo con %s.",
                       ext_lower, _parser_registry[ext_lower].__name__, parser_class.__name__)

    if not issubclass(parser_class, Parser):
         raise TypeError(f"La clase {parser_class.__name__} debe heredar de Parser.")

    _parser_registry[ext_lower] = parser_class
    logger.debug("Parser %s registrado para la extensión %s", parser_class.__name__, ext_lower)
    
# --- Función Principal get_parser ---
def get_parser(file_path: Path) -> Optional[Parser]:
    """
    Obtiene una instancia del parser adecuado para el archivo.
    Prioriza el nombre de archivo 'Dockerfile' y luego la extensión.

    Args:
        file_path: Ruta al archivo.

    Returns:
        Una instancia del Parser registrado, o None si no hay parser adecuado.
    """
    filename = file_path.name
    extension = file_path.suffix.lower()
    parser_class: Optional[Type[Parser]] = None

    # --- Comprobación Especial por Nombre de Archivo ---
    if filename.lower() == 'dockerfile':
        # Buscar la clase DockerfileParser por su nombre o una clave especial
        # Asumimos que DockerfileParser se registró con alguna extensión (ej. .dockerfile)
        # o podríamos buscarla explícitamente por nombre de clase si fuera necesario.
        # Lo más simple es buscarla si está registrada bajo '.dockerfile'
        parser_class = _parser_registry.get('.dockerfile') # Buscar por la extensión registrada
        if parser_class:
            logger.debug("Detectado archivo por nombre 'Dockerfile', usando parser registrado para '.dockerfile': %s", file_path)
        else:
            # Si no se registró con .dockerfile, necesitaríamos otro mecanismo
            # Podríamos importar dinámicamente o buscar en globals() pero es más complejo.
            # Por ahora, asumimos que debe registrarse con .dockerfile.
            logger.warning("Archivo llamado 'Dockerfile' encontrado, pero ningún parser está registrado para la extensión '.dockerfile'.")


    # --- Comprobación por Extensión (si no se encontró por nombre 'Dockerfile') ---
    if parser_class is None and extension:
        parser_class = _parser_registry.get(extension)
        if parser_class:
             logger.debug("Parser %s encontrado por extensión '%s' para %s",
                          parser_class.__name__, extension, file_path)
        # else: # Log ya hecho si se buscó por extensión
        #     logger.debug("No se encontró parser registrado para la extensión '%s' de %s",
        #                  extension, file_path)


    # --- Fallback a GenericTextParser ---
    if parser_class is None:
        if not is_likely_binary(file_path):
            try:
                # Importar generic aquí dentro
                from . import generic
                parser_class = generic.GenericTextParser
                logger.debug("No se encontró parser específico para %s y no parece binario. Usando GenericTextParser.", file_path)
            except ImportError:
                logger.error("No se pudo importar GenericTextParser. Fallback no disponible.")
        else:
             logger.debug("Archivo %s parece binario y no tiene parser específico. Omitiendo parsing.", file_path)

    # --- Instanciación ---
    if parser_class:
        try:
            return parser_class()
        except Exception as e:
             logger.error("Error instanciando parser %s: %s", parser_class.__name__, e, exc_info=True)
             return None
    else:
        return None
    
# --- Importación dinámica o registro explícito ---
# Ahora, en cada archivo de parser específico (ej. repogpt/parsers/python.py), harías algo como:
#
# from .base import Parser, register_parser
# import ast
#
# class PythonParser(Parser):
#     def parse(self, file_path, file_info):
#         # ... lógica de parsing con AST ...
#         content = file_path.read_text(...)
#         # ...
#         return {'classes': [...], 'functions': [...]}
#
# # Registrar el parser al importar el módulo
# register_parser('.py', PythonParser)

# Para que esto funcione, necesitarías importar todos los módulos de parser
# en algún punto central, por ejemplo, en repogpt/parsers/__init__.py:
#
# # repogpt/parsers/__init__.py
# from . import python
# from . import markdown
# from . import javascript
# # ... etc. para todos los parsers que crees
#
# from .base import get_parser # Re-exportar la función principal si se desea
# ==================== FIN: ./parsers/base.py ======================

# ==================== INICIO: ./parsers/python.py ====================
# ==================== INICIO: ./parsers/python.py ====================
# repogpt/parsers/python.py

import ast
import logging
from pathlib import Path
from typing import Any, Dict, List, Union
import tokenize


from .base import Parser, register_parser
from ..utils.text_processing import extract_todos_fixmes_from_comments, count_blank_lines

logger = logging.getLogger(__name__)

def get_comment_text(comment_token: tokenize.TokenInfo) -> str:
    """Extrae el texto limpio de un token de comentario."""
    text = comment_token.string.lstrip('#').strip()
    # Opcional: Podrías quitar prefijos comunes como '#:', '# ' si quieres
    return text

class PythonParser(Parser):
    """
    Parsea archivos Python (.py) usando ast y tokenize.
    Extrae una estructura ordenada de elementos: imports, comentarios,
    clases (con métodos y docstrings) y funciones (con docstrings).
    """

    def parse(self, file_path: Path, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """Analiza el archivo Python para extraer su estructura ordenada."""
        logger.debug("Parseando archivo Python: %s", file_path)
        result: Dict[str, Any] = {'structure': [], 'todos_fixmes': {'todos': [], 'fixmes': []}}
        content: str = ""
        try:
            # Leer contenido primero para tokenize y ast
            with file_path.open('rb') as fp:
                # Tokenize para obtener comentarios y su posición
                tokens = list(tokenize.tokenize(fp.readline))
            # Necesitamos el contenido como string para AST
            content = file_path.read_text(encoding='utf-8', errors='replace')
            file_info['line_count'] = len(content.splitlines())

            # Extraer comentarios y contar líneas en blanco usando tokens
            comments = []
            # blank_lines = 0
            # raw_lines = content.splitlines() # Necesario para contar líneas en blanco que tokenize ignora a veces
            # all_todos: List[Dict[str, Any]] = []
            # all_fixmes: List[Dict[str, Any]] = []
            # comment_pattern = re.compile(r'(TODO|FIXME)[\s:]?(.*)', re.IGNORECASE)

            for token in tokens:
                if token.type == tokenize.COMMENT:
                    comments.append({
                        'type': 'comment', # Mantener tipo por ahora si se usa
                        'line': token.start[0],
                        'text': get_comment_text(token) # Usar la función existente
                    })
                # Contar NL y líneas vacías asociadas (puede ser menos preciso que splitlines)
                # if token.type == tokenize.NL: # NL token indica fin de línea lógica
                #    if token.line and not token.line.strip(): # Si la línea asociada está vacía
                #         blank_lines += 1
                # elif token.type == tokenize.NEWLINE: # NEWLINE indica fin de stmt, puede haber líneas vacías
                #     # Este enfoque es más complejo con tokenize, usemos splitlines
                #     pass

            # Usar splitlines para contar líneas en blanco de forma fiable
            result['blank_lines'] = count_blank_lines(content)

            # Extraer TODOs/FIXMEs de los comentarios extraídos
            result['todos_fixmes'] = extract_todos_fixmes_from_comments(comments)
            result['comments_count'] = len(comments) # Conteo total de comentarios '#'

            # Parsear con AST
            tree = ast.parse(content, filename=str(file_path))

            # Combinar elementos AST y comentarios
            structure_elements = []
            for node in tree.body: # Iterar solo sobre los nodos de nivel superior
                if isinstance(node, (ast.Import, ast.ImportFrom)):
                    structure_elements.append(self._parse_import(node))
                elif isinstance(node, ast.ClassDef):
                    structure_elements.append(self._parse_class(node))
                elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    structure_elements.append(self._parse_function(node))
                # Ignorar otros tipos de nodos de nivel superior por ahora (Assign, Expr, etc.)
                # Podríamos añadir nodos 'code_block' para código suelto si fuera necesario.

            # Fusionar comentarios y elementos AST, luego ordenar por línea de inicio
            combined_elements = comments + structure_elements
            # Asegurar que todos tengan line_start antes de ordenar
            valid_elements = [elem for elem in combined_elements if 'line_start' in elem]
            result['structure'] = sorted(valid_elements, key=lambda x: x['line_start'])

            # Limpiar campos antiguos para evitar confusión (opcional pero recomendado)
            result.pop('classes', None)
            result.pop('functions', None)
            result.pop('imports', None)


        except FileNotFoundError:
            logger.error("Archivo no encontrado durante parsing: %s", file_path)
            return {'_error': 'File not found during parse'}
        except tokenize.TokenError as e:
            logger.warning("Error de tokenización en %s: %s", file_path, e)
            result['_error'] = f'Tokenization error: {e}'
            # Intentar parsear AST igualmente si es posible? O fallar aquí? Fallar es más seguro.
            return result
        except UnicodeDecodeError:
             logger.error("Error de decodificación en %s", file_path)
             return {'_error': 'Unicode decode error'}
        except SyntaxError as e:
            logger.warning("Error de sintaxis AST en %s: %s", file_path, e)
            result['_error'] = f'Syntax error: line {e.lineno}, offset {e.offset}: {e.msg}'
            result['structure'] = []
            # Mantener comentarios si se extrajeron antes del error
            existing_comments = [elem for elem in result.get('structure', []) if elem.get('type') == 'comment']
            result['structure'] = sorted(existing_comments, key=lambda x: x['line_start'])

        except Exception as e:
            logger.error("Error inesperado parseando Python %s: %s", file_path, e, exc_info=True)
            result['_error'] = f'Unexpected Python parsing error: {e}'
            result['structure'] = []

        return result

    def _get_node_end_lineno(self, node: ast.AST) -> int:
        """Obtiene la línea final de un nodo AST, manejando nodos sin end_lineno."""
        return getattr(node, 'end_lineno', node.lineno)

    def _parse_import(self, node: Union[ast.Import, ast.ImportFrom]) -> Dict[str, Any]:
        """Extrae información de una declaración de import."""
        if isinstance(node, ast.Import):
            value = f"import {', '.join(alias.name + (f' as {alias.asname}' if alias.asname else '') for alias in node.names)}"
        else: 
            module_name = node.module or '.' * node.level
            imports = ', '.join(alias.name + (f' as {alias.asname}' if alias.asname else '') for alias in node.names)
            value = f"from {module_name} import {imports}"
        return {
            'type': 'import',
            'line_start': node.lineno,
            'line_end': self._get_node_end_lineno(node),
            'value': value
        }

    def _parse_class(self, node: ast.ClassDef) -> Dict[str, Any]:
        """Extrae información detallada de una definición de clase y su cuerpo."""
        class_info = {
            'type': 'class',
            'name': node.name,
            'docstring': ast.get_docstring(node, clean=False) or None, # Mantener indentación original
            'line_start': node.lineno,
            'line_end': self._get_node_end_lineno(node),
            'bases': self._get_qual_names(node.bases),
            'decorators': self._get_decorator_names(node.decorator_list),
            'body': []
        }
        # Parsear cuerpo de la clase (métodos, asignaciones, comentarios internos, etc.)
        # Nota: Los comentarios dentro de la clase ya fueron extraídos por tokenize - Aquí solo procesamos nodos AST dentro de la clase.
        body_elements = []
        for item in node.body:
            if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                 # Pasar clean=False para mantener la indentación original del docstring
                body_elements.append(self._parse_function(item, is_method=True, clean_docstring=False))
            # Podríamos añadir parsing para asignaciones de atributos de clase aquí si es necesario
            # elif isinstance(item, ast.Assign):
            #     body_elements.append({'type': 'assignment', ...})
            # elif isinstance(item, ast.Pass): continue # Ignorar pass
            # else: # Otros nodos (Expr, etc.)
            #     try: body_elements.append({'type': 'code', 'line_start': item.lineno, 'line_end': self._get_node_end_lineno(item), 'value': ast.unparse(item)})
            #     except: pass # Ignorar si no se puede unparse

        class_info['body'] = sorted(body_elements, key=lambda x: x['line_start'])
        return class_info

    def _parse_function(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef], is_method=False, clean_docstring=True) -> Dict[str, Any]:
        """Extrae información detallada de una definición de función/método."""
        func_type = 'method' if is_method else 'function'
        return {
            'type': func_type,
            'name': node.name,
            'docstring': ast.get_docstring(node, clean=clean_docstring) or None,
            'line_start': node.lineno,
            'line_end': self._get_node_end_lineno(node),
            'arguments': self._format_arguments(node.args),
            'returns': ast.unparse(node.returns) if hasattr(node, 'returns') and node.returns and hasattr(ast, 'unparse') else None,
            'is_async': isinstance(node, ast.AsyncFunctionDef),
            'decorators': self._get_decorator_names(node.decorator_list)
        }

    def _get_decorator_names(self, decorator_list: List[ast.expr]) -> List[str]:
         """Intenta obtener los nombres de los decoradores usando ast.unparse."""
         names = []
         for d in decorator_list:
             try:
                 # ast.unparse (Python 3.9+) da la representación más fiel
                 names.append(f"@{ast.unparse(d)}")
             except AttributeError:
                 if isinstance(d, ast.Name):
                    names.append(f"@{d.id}")
                 elif isinstance(d, ast.Call) and isinstance(d.func, ast.Name):
                    names.append(f"@{d.func.id}(...)")
                 else: 
                    names.append(f"@decorator(...)")
         return names

    def _get_qual_names(self, nodes: List[ast.expr]) -> List[str]:
         """Intenta obtener nombres calificados (ej. module.Class) usando ast.unparse."""
         names = []
         for node in nodes:
             try:
                  names.append(ast.unparse(node)) # Requiere Python 3.9+
             except AttributeError:
                tmp = node.id if isinstance(node, ast.Name) else "complex_base"
                names.append(tmp)
         return names

    def _format_arguments(self, args: ast.arguments) -> List[str]:
        """Formatea los argumentos de una función/método a una lista de strings."""
        formatted_args = []
        # all_args = args.posonlyargs + args.args + args.kwonlyargs # Combinar posonlyargs, args, kwonlyargs

        # Obtener valores por defecto (requiere mapeo inverso)
        defaults = args.defaults
        kw_defaults = args.kw_defaults
        num_args_with_defaults = len(defaults)
        num_pos_kw_args = len(args.args)

        # Mapear args normales con sus defaults (desde el final)
        arg_defaults = {}
        if num_args_with_defaults > 0:
            for i in range(num_args_with_defaults):
                arg_index = num_pos_kw_args - num_args_with_defaults + i
                if arg_index >= 0 and arg_index < len(args.args):
                     arg_defaults[args.args[arg_index].arg] = defaults[i]

        # Mapear kwonly args con sus defaults
        kwonly_defaults = {}
        for i, arg in enumerate(args.kwonlyargs):
             if kw_defaults[i] is not None:
                 kwonly_defaults[arg.arg] = kw_defaults[i]

        arg_index_counter = 0
        # Positional-only arguments
        for arg in args.posonlyargs:
            arg_str = arg.arg
            if arg.annotation: 
                arg_str += f": {ast.unparse(arg.annotation)}" if hasattr(ast,'unparse') else ": Type"
            # Defaults para posonly son parte de 'defaults' antes que los de 'args'
            if arg_index_counter < len(defaults) - num_args_with_defaults :
                try: 
                    arg_str += f" = {ast.unparse(defaults[arg_index_counter])}" if hasattr(ast,'unparse') else " = ..."
                except: 
                    arg_str += " = ..."
            formatted_args.append(arg_str)
            arg_index_counter += 1

        if args.posonlyargs: 
            formatted_args.append('/')
        # Regular arguments
        for arg in args.args:
            arg_str = arg.arg
            if arg.annotation: 
                arg_str += f": {ast.unparse(arg.annotation)}" if hasattr(ast,'unparse') else ": Type"
            if arg.arg in arg_defaults:
                try: 
                    arg_str += f" = {ast.unparse(arg_defaults[arg.arg])}" if hasattr(ast,'unparse') else " = ..."
                except: 
                    arg_str += " = ..."
            formatted_args.append(arg_str)

        # Vararg (*args)
        if args.vararg:
            arg_str = f"*{args.vararg.arg}"
            if args.vararg.annotation:
                arg_str += f": {ast.unparse(args.vararg.annotation)}" if hasattr(ast,'unparse') else ": Type"
            formatted_args.append(arg_str)

        # Keyword-only arguments
        if args.kwonlyargs and not args.vararg:
             formatted_args.append('*') # Separador si no hay *args
        for arg in args.kwonlyargs:
            arg_str = arg.arg
            if arg.annotation: 
                arg_str += f": {ast.unparse(arg.annotation)}" if hasattr(ast,'unparse') else ": Type"
            if arg.arg in kwonly_defaults:
                try: 
                    arg_str += f" = {ast.unparse(kwonly_defaults[arg.arg])}" if hasattr(ast,'unparse') else " = ..."
                except:
                    arg_str += " = ..."
            formatted_args.append(arg_str)

        # Kwarg (**kwargs)
        if args.kwarg:
            arg_str = f"**{args.kwarg.arg}"
            if args.kwarg.annotation:
                arg_str += f": {ast.unparse(args.kwarg.annotation)}" if hasattr(ast,'unparse') else ": Type"
            formatted_args.append(arg_str)

        return formatted_args


# Registrar el parser
register_parser('.py', PythonParser)
# ==================== FIN: ./parsers/python.py ======================
# ==================== FIN: ./parsers/python.py ======================

# ==================== INICIO: ./parsers/markdown.py ====================
# repogpt/parsers/markdown.py
import logging
import re
from pathlib import Path
from typing import Any, Dict

from .base import Parser, register_parser
from ..utils.text_processing import extract_comments_from_content, extract_todos_fixmes_from_comments, count_blank_lines

logger = logging.getLogger(__name__)

class MarkdownParser(Parser):
    """Parsea archivos Markdown (.md, .mdx)."""

    def parse(self, file_path: Path, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """Extrae encabezados, links y fragmentos de código."""
        logger.debug("Parseando archivo Markdown: %s", file_path)
        result: Dict[str, Any] = {'headings': [], 'links': [], 'code_blocks': 0, 'content_preview': ''}
        try:
            content = file_path.read_text(encoding='utf-8', errors='replace')
            file_info['line_count'] = len(content.splitlines())

            # Extraer encabezados (simplificado)
            heading_pattern = re.compile(r'^(#+)\s+(.*)', re.MULTILINE)
            for match in heading_pattern.finditer(content):
                level = len(match.group(1))
                title = match.group(2).strip()
                result['headings'].append({'level': level, 'title': title})

            # Extraer links [text](url)
            link_pattern = re.compile(r'\[([^\]]+)\]\(([^)]+)\)')
            for match in link_pattern.finditer(content):
                text = match.group(1)
                url = match.group(2)
                result['links'].append({'text': text, 'url': url})

            # Contar bloques de código (```)
            code_block_pattern = re.compile(r'^```', re.MULTILINE)
            result['code_blocks'] = len(code_block_pattern.findall(content)) // 2 # Cada bloque tiene inicio y fin

            # Vista previa del contenido
            result['content_preview'] = content[:500] + ('...' if len(content) > 500 else '')
            result['blank_lines'] = count_blank_lines(content)

            # Extraer comentarios HTML <!-- ... --> y TODOs/FIXMEs
            # Patrón básico para comentarios HTML
            # No tenemos un patrón de comentario de línea estándar en Markdown puro
            comments = extract_comments_from_content(content, block_comment_patterns=[('<!--', '-->')])
            result['comments_count'] = len(comments)
            result['todos_fixmes'] = extract_todos_fixmes_from_comments(comments)
        except Exception as e:
            logger.error("Error parseando Markdown %s: %s", file_path, e, exc_info=True)
            result['_error'] = f'Unexpected Markdown parsing error: {e}'

        return result
    
# Registrar para .md y .mdx
register_parser('.md', MarkdownParser)
register_parser('.mdx', MarkdownParser) # MDX puede contener sintaxis adicional
# ==================== FIN: ./parsers/markdown.py ======================

# ==================== INICIO: ./parsers/javascript.py ====================
# repogpt/parsers/javascript.py
import logging
import re
from pathlib import Path
from typing import Any, Dict, Set

from .base import Parser, register_parser
from ..utils.text_processing import extract_comments_from_content, extract_todos_fixmes_from_comments, count_blank_lines

logger = logging.getLogger(__name__)

try:
    from pyjsparser import PyJsParser, JsSyntaxError # Intentar importar pyjsparser
    pyjsparser_available = True
    logger.debug("pyjsparser encontrado. Se usará para análisis AST de JS/TS.")
except ImportError:
    PyJsParser = None
    JsSyntaxError = None
    pyjsparser_available = False
    logger.warning("pyjsparser no instalado. El análisis de JS/TS se basará en Regex (menos preciso).")
    logger.warning("Instala con: pip install repogpt[js]")

class JavaScriptParser(Parser):
    """Parsea archivos JavaScript/TypeScript (.js, .jsx, .ts, .tsx)."""

    def parse(self, file_path: Path, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """Intenta usar AST si está disponible, si no, usa Regex."""
        logger.debug("Parseando archivo JS/TS: %s", file_path)
        result: Dict[str, Any] = {
            'imports': [], 'exports': [], 'functions': [],
            'classes': [], 'components': [], 'todos_fixmes': {}
        }
        try:
            content = file_path.read_text(encoding='utf-8', errors='replace')
            file_info['line_count'] = len(content.splitlines())

            if pyjsparser_available:
                try:
                    parser = PyJsParser()
                    # Nota: pyjsparser puede tener problemas con sintaxis moderna (TS, JSX)
                    # Podría ser necesario un parser más robusto (ej. esprima-python, o llamar a node/babel externo)
                    parsed_ast = parser.parse(content)
                    # --- Implementar extracción desde el AST de pyjsparser ---
                    # Esto es un placeholder, se necesitaría recorrer el AST ('body', etc.)
                    # para encontrar declaraciones de funciones, clases, exports, imports...
                    # Ejemplo muy básico (no funcional):
                    # for node in parsed_ast.get('body', []):
                    #    if node.get('type') == 'FunctionDeclaration':
                    #        result['functions'].append(node.get('id', {}).get('name'))
                    #    elif node.get('type') == 'ExportNamedDeclaration':
                    #        # ... extraer exports
                    #    elif node.get('type') == 'ImportDeclaration':
                    #        # ... extraer imports
                    result['ast_parsed'] = True # Indicar que se usó AST
                    # logger.warning("Extracción AST desde pyjsparser aún no implementada detalladamente.")
                except (JsSyntaxError, RecursionError, Exception) as ast_err:
                    # Manejar errores de parsing AST o si pyjsparser falla con sintaxis compleja
                    logger.warning("Fallo al parsear %s con pyjsparser AST (%s). Usando Regex como fallback.", file_path, ast_err)
                    result.update(self._parse_with_regex(content))
                    result['ast_parse_error'] = str(ast_err)
            else:
                # Usar Regex si pyjsparser no está disponible
                result.update(self._parse_with_regex(content))

                result['blank_lines'] = count_blank_lines(content)

                # Definir patrones de comentario JS
                js_line_patterns = [r'//\s*(.*)']
                js_block_patterns = [('/*', '*/')] # Tupla de strings literales

                # Extraer comentarios y TODOs/FIXMEs
                comments = extract_comments_from_content(content,
                                                        line_comment_patterns=js_line_patterns,
                                                        block_comment_patterns=js_block_patterns)
                result['comments_count'] = len(comments)
                result['todos_fixmes'] = extract_todos_fixmes_from_comments(comments)

                # Continuar con el parsing AST o Regex...
                if pyjsparser_available:
                    # ... (lógica AST)
                    pass # Asegúrate de que el código AST no dependa de los métodos eliminados
                else:
                    result.update(self._parse_with_regex(content))
                    # Asegúrate de que _parse_with_regex no dependa de los métodos eliminados
     
        except Exception as e:
            logger.error("Error inesperado parseando JS/TS %s: %s", file_path, e, exc_info=True)
            result['_error'] = f'Unexpected JS/TS parsing error: {e}'

        return result

    def _parse_with_regex(self, content: str) -> Dict[str, Any]:
        """Análisis basado en Regex para JS/TS (menos preciso)."""
        regex_result: Dict[str, Any] = {'imports': [], 'exports': [], 'functions': [], 'classes': [], 'components': []}
        imports: Set[str] = set()
        exports: Set[str] = set()
        functions: Set[str] = set()
        classes: Set[str] = set()
        components: Set[str] = set()

        # Imports (simplificado)
        # import ... from '...' / import '...' / require('...')
        import_pattern = re.compile(r'import(?:[\s\S]*?)from\s+[\'"]([^\'"]+)[\'"]|import\s+[\'"]([^\'"]+)[\'"]|require\s*\(\s*[\'"]([^\'"]+)[\'"]\s*\)', re.MULTILINE)
        for match in import_pattern.finditer(content):
            imports.add(match.group(1) or match.group(2) or match.group(3))

        # Exports (simplificado)
        # export function ..., export class ..., export const ..., export default ...
        export_pattern = re.compile(r'export\s+(?:default\s+)?(?:function|class|const|let|var)\s+([a-zA-Z_$][\w$]*)', re.MULTILINE)
        exports.update(export_pattern.findall(content))

        # Functions (simplificado - function name(...) or const name = (...) => {...})
        func_pattern = re.compile(r'(?:function\s+([a-zA-Z_$][\w$]*)\s*\(|const\s+([a-zA-Z_$][\w$]*)\s*=\s*(?:async\s*)?\([\s\S]*?\)\s*=>)', re.MULTILINE)
        for match in func_pattern.finditer(content):
            functions.add(match.group(1) or match.group(2))

        # Classes (simplificado)
        class_pattern = re.compile(r'class\s+([a-zA-Z_$][\w$]*)(?:\s+extends\s+[\w$.]+)?', re.MULTILINE)
        classes.update(class_pattern.findall(content))

        # React Components (simplificado - empieza con Mayúscula)
        # Podría coincidir con clases normales, necesita refinamiento
        component_pattern = re.compile(r'(?:function|class)\s+([A-Z]\w*)\s*(?:\(|\{|extends)', re.MULTILINE)
        components.update(component_pattern.findall(content))

        regex_result['imports'] = sorted(list(imports))
        regex_result['exports'] = sorted(list(exports))
        regex_result['functions'] = sorted(list(functions))
        regex_result['classes'] = sorted(list(classes))
        regex_result['components'] = sorted(list(components))
        regex_result['parsed_with'] = 'regex'
        return regex_result



# Registrar para extensiones JS/TS/JSX/TSX
register_parser('.js', JavaScriptParser)
register_parser('.jsx', JavaScriptParser)
register_parser('.ts', JavaScriptParser)
register_parser('.tsx', JavaScriptParser)
# ==================== FIN: ./parsers/javascript.py ======================

# ==================== INICIO: ./parsers/html.py ====================
# repogpt/parsers/html.py
import logging
import re
from pathlib import Path
from typing import Any, Dict

from .base import Parser, register_parser
from ..utils.text_processing import extract_comments_from_content, extract_todos_fixmes_from_comments, count_blank_lines


logger = logging.getLogger(__name__)

class HtmlParser(Parser):
    """Parsea archivos HTML (.html, .htm)."""

    def parse(self, file_path: Path, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """Extrae título, scripts y links."""
        logger.debug("Parseando archivo HTML: %s", file_path)
        result: Dict[str, Any] = {'title': '', 'scripts': [], 'links': [], 'style_blocks': 0}
        try:
            content = file_path.read_text(encoding='utf-8', errors='replace')
            file_info['line_count'] = len(content.splitlines())
            result['blank_lines'] = count_blank_lines(content)

            # Extraer comentarios HTML y TODOs/FIXMEs
            comments = extract_comments_from_content(content, block_comment_patterns=[('<!--', '-->')])
            result['comments_count'] = len(comments)
            result['todos_fixmes'] = extract_todos_fixmes_from_comments(comments)

            # Extraer título
            title_match = re.search(r'<title>(.*?)</title>', content, re.IGNORECASE | re.DOTALL)
            result['title'] = title_match.group(1).strip() if title_match else 'N/A'

            # Extraer fuentes de scripts <script src="...">
            script_pattern = re.compile(r'<script[^>]+src\s*=\s*["\']([^"\']+)["\']', re.IGNORECASE)
            result['scripts'] = script_pattern.findall(content)

            # Extraer HREFs de links <a href="..."> y stylesheets <link href="...">
            link_pattern = re.compile(r'<(?:a|link)[^>]+href\s*=\s*["\']([^"\']+)["\']', re.IGNORECASE)
            result['links'] = link_pattern.findall(content)

            # Contar bloques <style>...</style>
            style_pattern = re.compile(r'<style.*?>.*?</style>', re.IGNORECASE | re.DOTALL)
            result['style_blocks'] = len(style_pattern.findall(content))
            
        except Exception as e:
            logger.error("Error inesperado parseando HTML %s: %s", file_path, e, exc_info=True)
            result['_error'] = f'Unexpected HTML parsing error: {e}'

        return result


register_parser('.html', HtmlParser)
register_parser('.htm', HtmlParser)
# ==================== FIN: ./parsers/html.py ======================

# ==================== INICIO: ./parsers/generic.py ====================
# repogpt/parsers/generic.py
import logging
from pathlib import Path
from typing import Any, Dict, List

from .base import Parser # No se registra, es un fallback
# Importar utils actualizadas
from ..utils.text_processing import (
    extract_comments_from_content,
    extract_todos_fixmes_from_comments
)


logger = logging.getLogger(__name__)

class GenericTextParser(Parser):
    """Parser genérico para archivos de texto no reconocidos por otros parsers."""

    def parse(self, file_path: Path, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """Extrae información básica como conteo de líneas y TODOs/FIXMEs usando streaming."""
        logger.debug("Parseando archivo genérico (streaming): %s", file_path)
        result: Dict[str, Any] = {'content_preview': '', 'todos_fixmes': {}, 'comments_count': 0, 'blank_lines': 0}
        lines_buffer: List[str] = []
        line_count = 0
        preview_char_count = 0
        PREVIEW_LIMIT = 500

        try:
            with file_path.open('r', encoding='utf-8', errors='replace') as f:
                for line in f:
                    # --- Procesamiento línea por línea ---
                    line_count += 1
                    if not line.strip():
                        result['blank_lines'] += 1

                    # Almacenar para preview (solo lo necesario)
                    if preview_char_count < PREVIEW_LIMIT:
                         lines_buffer.append(line)
                         preview_char_count += len(line)

                    # Podríamos aplicar regex de comentarios/todos aquí línea por línea
                    # pero extract_comments_from_content necesita el contenido completo.
                    # Mantenemos el enfoque actual por simplicidad, pero sacrificando
                    # el beneficio completo del streaming para estas extracciones.
                    # Si la memoria fuera crítica, habría que refactorizar las utils
                    # para que acepten iteradores o procesen línea a línea.

            file_info['line_count'] = line_count

            # Reconstruir contenido solo si es necesario (para preview y análisis completo)
            # Si el archivo es muy grande, esto anula parcialmente el beneficio del stream
            # Podríamos limitar la reconstrucción o el análisis posterior.
            content = "".join(lines_buffer) # Contenido parcial para preview
            if preview_char_count >= PREVIEW_LIMIT:
                 # Si leímos más allá del límite, truncar y añadir puntos
                 # Necesitaríamos leer el archivo completo para hacer bien el análisis de comentarios/todos
                 # Aquí hacemos un compromiso: analizamos solo el preview
                 logger.warning("Analizando solo el preview (%d bytes) para comentarios/todos en archivo grande %s",
                                PREVIEW_LIMIT, file_path)
                 result['content_preview'] = content[:PREVIEW_LIMIT] + '...'
                 # Leer el archivo completo para un análisis preciso (¡anula el streaming!)
                 # content_full = file_path.read_text(encoding='utf-8', errors='replace')
                 # O aceptar el análisis sobre el preview:
                 content_for_analysis = content[:PREVIEW_LIMIT]

            else:
                 result['content_preview'] = content
                 content_for_analysis = content # El preview es el contenido completo

            # Intentar extraer TODOs/FIXMEs genéricos (buscar en comentarios #, //)
            # Usar content_for_analysis (que puede ser solo el preview)
            generic_line_patterns = [r'^\s*#\s*(.*)', r'^\s*//\s*(.*)']
            comments = extract_comments_from_content(content_for_analysis, line_comment_patterns=generic_line_patterns)
            result['comments_count'] = len(comments)
            result['todos_fixmes'] = extract_todos_fixmes_from_comments(comments)


        except FileNotFoundError:
             logger.error("Archivo no encontrado durante parsing genérico: %s", file_path)
             return {'_error': f'File not found: {file_path}'}
        except Exception as e:
            logger.error("Error inesperado parseando archivo genérico %s: %s", file_path, e, exc_info=True)
            result['_error'] = f'Unexpected generic parsing error: {e}'
            # Poner line_count si se alcanzó a calcular
            if line_count > 0: file_info['line_count'] = line_count


        return result
# ==================== FIN: ./parsers/generic.py ======================

# ==================== INICIO: ./parsers/yaml_parser.py ====================
# repogpt/parsers/yaml_parser.py
import logging
from pathlib import Path
from typing import Any, Dict

from .base import Parser, register_parser
from ..utils.text_processing import extract_comments_from_content, extract_todos_fixmes_from_comments, count_blank_lines

logger = logging.getLogger(__name__)

# Dependencia opcional
try:
    import yaml
    pyyaml_available = True
    logger_yaml = logging.getLogger(__name__) # Logger específico si queremos
    logger_yaml.debug("PyYAML encontrado.")
except ImportError:
    yaml = None
    pyyaml_available = False
    logging.warning("PyYAML no instalado. No se parsearán archivos YAML.")
    logging.warning("Instala con: pip install repogpt[yaml]") # Asumiendo extra 'yaml'

class YamlParser(Parser):
    """Parsea archivos YAML (.yaml, .yml)."""

    def parse(self, file_path: Path, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """Intenta cargar el archivo YAML y extrae claves principales."""
        logger.debug("Parseando archivo YAML: %s", file_path)
        result: Dict[str, Any] = {'top_level_keys': [], 'structure_preview': None}

        if not pyyaml_available:
            result['_error'] = 'PyYAML library not installed'
            return result

        try:
            content = file_path.read_text(encoding='utf-8', errors='replace')
            file_info['line_count'] = len(content.splitlines())

            # Cargar el documento YAML
            data = yaml.safe_load(content)

            if isinstance(data, dict):
                result['top_level_keys'] = list(data.keys())
                # Crear una vista previa de la estructura (simplificada)
                result['structure_preview'] = self._get_structure_preview(data)
            elif isinstance(data, list):
                 result['top_level_keys'] = [f"list_item_{i}" for i in range(min(len(data), 5))] # Max 5 items
                 result['structure_preview'] = 'List'
            else:
                 result['structure_preview'] = type(data).__name__ # Scalar?

            result['blank_lines'] = count_blank_lines(content)

            # Extraer comentarios YAML (#) y TODOs/FIXMEs
            yaml_line_patterns = [r'^\s*#\s*(.*)'] # Asegura que # esté al inicio (después de espacios)
            comments = extract_comments_from_content(content, line_comment_patterns=yaml_line_patterns)
            result['comments_count'] = len(comments)
            result['todos_fixmes'] = extract_todos_fixmes_from_comments(comments)


        except yaml.YAMLError as e:
             logger.warning("Error de sintaxis YAML en %s: %s", file_path, e)
             result['_error'] = f'YAML syntax error: {e}'
        except Exception as e:
            logger.error("Error inesperado parseando YAML %s: %s", file_path, e, exc_info=True)
            result['_error'] = f'Unexpected YAML parsing error: {e}'

        return result

    def _get_structure_preview(self, data: Any, depth=0, max_depth=2) -> Any:
        """Genera una vista previa de la estructura YAML/Dict."""
        if depth > max_depth:
            return '...'

        if isinstance(data, dict):
            preview = {}
            for k, v in data.items():
                preview[k] = self._get_structure_preview(v, depth + 1, max_depth)
            return preview
        elif isinstance(data, list):
             # Mostrar tipo de los primeros N elementos
             if not data: 
                 return []
             preview_items = [self._get_structure_preview(item, depth + 1, max_depth) for item in data[:3]] # Max 3 items
             return preview_items + (['...'] if len(data) > 3 else [])
        else:
            # Para valores escalares, solo mostrar el tipo
            return type(data).__name__

# Registrar si PyYAML está disponible
if pyyaml_available:
    register_parser('.yaml', YamlParser)
    register_parser('.yml', YamlParser)
# ==================== FIN: ./parsers/yaml_parser.py ======================

# ==================== INICIO: ./parsers/dockerfile_.py ====================
# repogpt/parsers/dockerfile.py
import logging
import re
from pathlib import Path
from typing import Any, Dict, List

# Importar desde .base para obtener Parser y register_parser
from .base import Parser, register_parser

logger = logging.getLogger(__name__)

class DockerfileParser(Parser):
    """Parsea archivos Dockerfile."""

    # Instrucciones comunes de Dockerfile
    # Lista más completa y ordenada alfabéticamente
    KNOWN_INSTRUCTIONS = sorted([
        'ADD', 'ARG', 'CMD', 'COPY', 'ENTRYPOINT', 'ENV', 'EXPOSE', 'FROM',
        'HEALTHCHECK', 'LABEL', 'MAINTAINER', # Maintainer está obsoleto pero puede aparecer
        'ONBUILD', 'RUN', 'SHELL', 'STOPSIGNAL', 'USER', 'VOLUME', 'WORKDIR'
    ])
    # Patrón mejorado para capturar instrucciones, incluyendo comentarios y continuación de línea
    # Busca la instrucción al inicio de línea (insensible a mayúsculas)
    # Maneja espacios y continuación de línea (\) antes de los argumentos
    INSTRUCTION_PATTERN = re.compile(
        rf'^\s*({"|".join(KNOWN_INSTRUCTIONS)})\s*(.*?)(\s*#.*)?$',
        re.IGNORECASE | re.MULTILINE
    )
    # Patrón específico para FROM para capturar imagen, tag, digest y alias
    FROM_PATTERN = re.compile(
        r'^\s*(--platform=\S+\s+)?([\w./\-:]+)(?:[:@]([\w.\-]+))?(?:\s+as\s+([\w.\-]+))?',
        re.IGNORECASE
    )
    # Patrón para comentarios de línea
    COMMENT_PATTERN = re.compile(r'^\s*#\s*(.*)')
    # Patrón para TODO/FIXME dentro de comentarios
    TODO_FIXME_PATTERN = re.compile(r'(TODO|FIXME)[\s:]?(.*)', re.IGNORECASE)


    def parse(self, file_path: Path, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """Extrae instrucciones, argumentos, imágenes base, puertos y tareas."""
        logger.debug("Parseando Dockerfile: %s", file_path)
        result: Dict[str, Any] = {
            'instructions': [],
            'base_images': [],
            'exposed_ports': [],
            'labels': {},
            'envs': {},
            'args_defined': [],
            'comments': [],
            'todos_fixmes': {'todos': [], 'fixmes': []}
        }
        try:
            # Leer todo el contenido manejando continuaciones de línea (escaped newlines)
            # Esto simplifica el parseo de argumentos multilínea en instrucciones RUN, etc.
            raw_content = file_path.read_text(encoding='utf-8', errors='replace')
            content = re.sub(r'\\\n', '', raw_content) # Eliminar continuaciones de línea

            file_info['line_count'] = len(raw_content.splitlines()) # Contar sobre el original

            current_line_num = 0 # Aproximado para errores/todos
            raw_lines = raw_content.splitlines()

            # Procesar línea por línea para comentarios y TODOs
            for i, line in enumerate(raw_lines):
                current_line_num = i + 1
                # Extraer comentarios
                comment_match = self.COMMENT_PATTERN.match(line)
                if comment_match:
                     text = comment_match.group(1).strip()
                     # Ignorar pragmas comunes
                     if not line.strip().startswith('# syntax='):
                          result['comments'].append({'line': current_line_num, 'text': text})
                          # Buscar TODOs/FIXMEs dentro de comentarios
                          todo_match = self.TODO_FIXME_PATTERN.search(text)
                          if todo_match:
                              marker = todo_match.group(1).upper()
                              message = todo_match.group(2).strip()
                              entry = {'line': current_line_num, 'message': message}
                              if marker == 'TODO':
                                  result['todos_fixmes']['todos'].append(entry)
                              else:
                                  result['todos_fixmes']['fixmes'].append(entry)

            # Procesar instrucciones usando el contenido sin continuaciones
            for match in self.INSTRUCTION_PATTERN.finditer(content):
                instruction = match.group(1).upper()
                # Los argumentos pueden contener espacios, quitar solo los extremos
                arguments = match.group(2).strip()

                # Guardar instrucción básica
                instruction_detail = {'instruction': instruction, 'arguments': arguments}
                # Intentar obtener línea aproximada (no es perfecto debido a continuaciones)
                # instruction_detail['line_approx'] = content[:match.start()].count('\n') + 1
                result['instructions'].append(instruction_detail)

                # Extraer info específica de instrucciones comunes
                if instruction == 'FROM':
                    from_match = self.FROM_PATTERN.match(arguments)
                    if from_match:
                        platform, image, tag_or_digest, alias = from_match.groups()
                        base_image_info = {'image': image}
                        if tag_or_digest:
                            # Determinar si es tag o digest (simplificado)
                            if len(tag_or_digest) > 15 and any(c in tag_or_digest for c in 'abcdef'): # Heurística para digest
                                base_image_info['digest'] = tag_or_digest
                            else:
                                base_image_info['tag'] = tag_or_digest
                        if alias:
                            base_image_info['alias'] = alias
                        if platform:
                             base_image_info['platform'] = platform.strip()
                        result['base_images'].append(base_image_info)
                    else:
                        # Fallback si el regex complejo falla
                        result['base_images'].append({'image': arguments.split(' ')[0], '_parse_warning': 'Complex FROM not fully parsed'})

                elif instruction == 'EXPOSE':
                    # Extraer puertos expuestos (números y opcionalmente /tcp o /udp)
                    ports = re.findall(r'\d+(?:/(?:tcp|udp))?', arguments, re.IGNORECASE)
                    result['exposed_ports'].extend(ports)

                elif instruction == 'LABEL':
                    # Parsear labels estilo clave="valor" clave2=valor2 ...
                    # Simplificado: asume pares separados por espacio, puede fallar con comillas
                    try:
                       # Intenta parsear como clave=valor (puede fallar con espacios en valores)
                       label_pairs = re.findall(r'([\w.-]+)\s*=\s*(?:"([^"]*)"|\'([^\']*)\'|(\S+))', arguments)
                       for key, v_dq, v_sq, v_nq in label_pairs:
                           result['labels'][key] = v_dq or v_sq or v_nq
                    except Exception:
                         result['labels']['_parse_warning'] = f"Could not fully parse LABEL arguments: {arguments}"

                elif instruction == 'ENV':
                     # Parsear ENV clave=valor clave valor ...
                     # Simplificado: asume un solo par o clave+valor
                     env_parts = arguments.split(None, 1)
                     if len(env_parts) == 1: # Estilo ENV clave valor
                          # Buscar clave=valor
                          if '=' in env_parts[0]:
                               key, value = env_parts[0].split('=', 1)
                               result['envs'][key.strip()] = value.strip().strip('"\'')
                          else: # Solo clave? Dockerfile permite esto pero es raro.
                              result['envs'][env_parts[0]] = '' # Asignar vacío
                     elif len(env_parts) == 2: # Estilo ENV clave valor
                          key, value = env_parts
                          result['envs'][key.strip()] = value.strip().strip('"\'')


                elif instruction == 'ARG':
                     # Extraer nombre del argumento (puede tener valor por defecto)
                     arg_name = arguments.split('=')[0].strip()
                     result['args_defined'].append(arg_name)

            # Actualizar el conteo de comentarios para reflejar solo los reales
            file_info['comments_count'] = len(result['comments'])


        except FileNotFoundError:
            logger.error("Archivo Dockerfile no encontrado durante parsing: %s", file_path)
            result['_error'] = 'File not found during parsing'
        except Exception as e:
            logger.error("Error inesperado parseando Dockerfile %s: %s", file_path, e, exc_info=True)
            result['_error'] = f'Unexpected Dockerfile parsing error: {e}'

        return result

# Registrar para la extensión .dockerfile
register_parser('.dockerfile_', DockerfileParser)

# Nota: La detección de archivos llamados 'Dockerfile' (sin extensión)
# se maneja ahora en parsers/base.py/get_parser
# ==================== FIN: ./parsers/dockerfile_.py ======================

# ==================== INICIO: ./reporting/__init__.py ====================
# repogpt/reporting/__init__.py

from . import json_reporter
from . import markdown_reporter # Asumiendo que se crea

from .base import Reporter

__all__ = [
    'Reporter',
    'json_reporter',
    'markdown_reporter',
]
# ==================== FIN: ./reporting/__init__.py ======================

# ==================== INICIO: ./reporting/base.py ====================
# repogpt/reporting/base.py

import abc
from typing import Any, Dict

class Reporter(abc.ABC):
    """Clase base abstracta para generadores de reportes."""

    @abc.abstractmethod
    def generate(self, analysis_data: Dict[str, Any]) -> str:
        """
        Genera el contenido del reporte a partir de los datos analizados.

        Args:
            analysis_data: El diccionario completo que contiene todos los datos
                           recopilados por el analizador y los extractores.

        Returns:
            Una cadena de texto que representa el reporte formateado.
        """
        pass
# ==================== FIN: ./reporting/base.py ======================

# ==================== INICIO: ./reporting/json_reporter.py ====================
import json
import logging
from typing import Any, Dict
import argparse # Importar argparse

from .base import Reporter
# Asegúrate de importar la excepción si no está ya
from repogpt.exceptions import ReportingError

logger = logging.getLogger(__name__)

class JsonReporter(Reporter):
    """Genera el reporte en formato JSON."""

    # Actualizar firma para aceptar report_options (args) aunque no los usemos aquí
    def generate(self, analysis_data: Dict[str, Any], report_options: argparse.Namespace) -> str:
        """Convierte los datos de análisis a una cadena JSON formateada."""
        logger.info("Generando reporte en formato JSON...")
        # Nota: Las opciones de filtrado (--summary, etc.) no se aplican al formato JSON.
        # El JSON siempre contendrá toda la información extraída.
        if report_options.summary or report_options.dependencies or report_options.tasks or report_options.file_metadata:
             logger.debug("Las opciones de filtrado de secciones no aplican al formato JSON.")

        try:
            report_content = json.dumps(analysis_data, indent=2, ensure_ascii=False, default=str)
            logger.info("Reporte JSON generado exitosamente.")
            return report_content
        except TypeError as e:
            logger.error("Error de serialización JSON: %s. Puede haber tipos de datos no serializables.", e)
            try:
                logger.warning("Intentando serialización JSON con conversión a string por defecto.")
                report_content = json.dumps(analysis_data, indent=2, ensure_ascii=False, default=str)
                return report_content
            except Exception as final_e:
                 logger.critical("Fallo final al generar reporte JSON: %s", final_e, exc_info=True)
                 raise ReportingError(f"No se pudo serializar a JSON: {final_e}") from final_e
        except Exception as e:
             logger.critical("Error inesperado generando reporte JSON: %s", e, exc_info=True)
             raise ReportingError(f"Error inesperado en JsonReporter: {e}") from e
# ==================== FIN: ./reporting/json_reporter.py ======================

# ==================== INICIO: ./reporting/markdown_reporter.py ====================
# repogpt/reporting/markdown_reporter.py

import logging
from typing import Any, Dict, List
import json
import argparse

from .base import Reporter

logger = logging.getLogger(__name__)

class MarkdownReporter(Reporter):
    """Genera el reporte en formato Markdown, incluyendo estructura detallada del código."""

    # El método generate ahora acepta 'report_options' (que será args)
    def generate(self, analysis_data: Dict[str, Any], report_options: argparse.Namespace) -> str:
        """Crea una cadena de texto Markdown a partir de los datos de análisis."""
        logger.info("Generando reporte Markdown con opciones: %s", report_options)
        report_lines: List[str] = ["# RepoGPT Analysis Report"]

        # --- 1. Resumen General (Opcional) ---
        if report_options.summary:
            report_lines.append("\n## Summary\n")
            git_info = analysis_data.get("git_info")
            metrics = analysis_data.get("code_metrics")

            if git_info:
                report_lines.append("### Git Information")
                report_lines.append(f"- **Branch:** `{git_info.get('branch', 'N/A')}`")
                report_lines.append(f"- **Commit:** `{git_info.get('commit_short_hash', 'N/A')}`")
                report_lines.append(f"- **Author:** {git_info.get('author_name', 'N/A')}")
                report_lines.append(f"- **Date:** {git_info.get('commit_date', 'N/A')}")
                if tags := git_info.get('tags'):
                    report_lines.append(f"- **Tags:** {', '.join(f'`{t}`' for t in tags)}")
                report_lines.append("")
            elif report_options.summary: # Si se pidió summary pero no hay datos git
                 report_lines.append("*(Git information not available or extractor disabled)*\n")


            if metrics:
                report_lines.append("### Code Metrics")
                report_lines.append(f"- **Total Files Analyzed:** {metrics.get('total_files', 0)}")
                report_lines.append(f"- **Total Lines:** {metrics.get('total_lines', 0)}")
                report_lines.append(f"- **Code Lines:** {metrics.get('code_lines', 0)}")
                report_lines.append(f"- **Comment Lines:** {metrics.get('total_comment_lines', 0)}")
                report_lines.append(f"- **Blank Lines:** {metrics.get('total_blank_lines', 0)}")
                report_lines.append(f"- **Total Size:** {self._format_size(metrics.get('total_size_bytes', 0))}")
                
                report_lines.append(f"- **Files with Errors:** {metrics.get('files_with_errors', 0)}")
                if metrics.get('files_missing_line_data', 0) > 0:
                    report_lines.append(f"- **Files without Line Data:** {metrics['files_missing_line_data']}")
                if ext_stats := metrics.get("files_by_extension"):
                    report_lines.append("- **File Types:**")
                    report_lines.append("  | Extension | Count |")
                report_lines.append("")
            elif report_options.summary: # Si se pidió summary pero no hay datos de métricas
                 report_lines.append("*(Code metrics not available or extractor disabled)*\n")


        # --- 2. Dependencias (Simple por defecto, Detallado opcional) ---
        dependencies_data = analysis_data.get("dependencies")
        if dependencies_data:
             if report_options.dependencies:
                 report_lines.append("\n## Dependencies (Detailed)\n")
                 for file_name, deps in sorted(dependencies_data.items()):
                     report_lines.append(f"### `{file_name}`")
                     if isinstance(deps, dict) and '_error' in deps:
                         report_lines.append(f"  - **Error parsing:** {deps['_error']}")
                     elif isinstance(deps, list) and deps and isinstance(deps[0], dict) and '_error' in deps[0]:
                          report_lines.append(f"  - **Error parsing:** {deps[0]['_error']}")
                     elif isinstance(deps, dict) or isinstance(deps, list):
                         try:
                              deps_str = json.dumps(deps, indent=2, ensure_ascii=False)
                              report_lines.append(f"```json\n{deps_str}\n```")
                         except TypeError:
                              report_lines.append(f"```\n{str(deps)}\n```")
                     else:
                         report_lines.append("  - *No dependencies found or unrecognized format.*")
                     report_lines.append("")
             # Comportamiento Simple (por defecto, si hay datos pero no se pide detalle)
             elif not report_options.dependencies:
                  detected_files = [f"`{fname}`" for fname, data in dependencies_data.items() if not (isinstance(data, dict) and '_error' in data)]
                  if detected_files:
                      report_lines.append("\n## Dependencies Found\n")
                      report_lines.append(f"- Files: {', '.join(detected_files)}")
                      report_lines.append("  *(Run with `--dependencies` for details)*")
                      report_lines.append("")

        # --- 3. Tareas Pendientes (Lista agregada opcional) ---
        tasks_data = analysis_data.get("aggregated_tasks")
        if tasks_data and report_options.tasks: # Solo mostrar si hay datos Y se pidió con --tasks
            total_todos = tasks_data.get("total_todos", 0)
            total_fixmes = tasks_data.get("total_fixmes", 0)
            if total_todos > 0 or total_fixmes > 0:
                report_lines.append(f"\n## Aggregated Tasks ({total_todos} TODOs, {total_fixmes} FIXMEs)\n")
                if todos_map := tasks_data.get("todos"):
                     self._append_task_section(report_lines, "TODOs", todos_map)
                if fixmes_map := tasks_data.get("fixmes"):
                     self._append_task_section(report_lines, "FIXMEs", fixmes_map)

        # --- 4. Detalles por Archivo (Metadata detallada opcional, conteo de tareas por defecto) ---
        if files_data := analysis_data.get("files"):
            report_lines.append("\n## File Details\n")
            sorted_files = sorted(files_data.items())
            # Obtener datos de tareas agregadas para los conteos por archivo
            aggregated_tasks = analysis_data.get("aggregated_tasks", {})
            todos_by_file = aggregated_tasks.get("todos", {})
            fixmes_by_file = aggregated_tasks.get("fixmes", {})


            for relative_path, file_info in sorted_files:
                if not isinstance(file_info, dict):
                    continue

                report_lines.append(f"### `{relative_path}`")

                # Metadata: Líneas siempre, resto opcional
                if 'line_count' in file_info:
                     report_lines.append(f"- **Lines:** {file_info['line_count']}")
                if report_options.file_metadata:
                    report_lines.append(f"- **Size:** {self._format_size(file_info.get('size', 0))}")
                    report_lines.append(f"- **Hash (SHA256):** `{file_info.get('hash', 'N/A')}`")

                # Conteo de Tareas por Archivo (siempre, si existen)
                num_todos = len(todos_by_file.get(relative_path, []))
                num_fixmes = len(fixmes_by_file.get(relative_path, []))
                if num_todos > 0 or num_fixmes > 0:
                    task_summary = []
                    if num_todos > 0:
                        task_summary.append(f"{num_todos} TODOs")
                    if num_fixmes > 0:
                        task_summary.append(f"{num_fixmes} FIXMEs")
                    report_lines.append(f"- **Tasks Found:** {', '.join(task_summary)}")
                    # Añadir nota si la lista completa no está visible
                    if not report_options.tasks:
                         report_lines.append("  *(Run with `--tasks` for full list)*")


                if error := file_info.get('_error'):
                    report_lines.append(f"- **<span style='color:red;'>Error:</span>** `{error}`")
                    if structure := file_info.get('structure'):
                        if structure:
                             report_lines.append("\n#### Partial Structure (before error):")
                             self._render_structure(report_lines, structure, indent_level=0)
                    report_lines.append("\n---\n")
                    continue

                # Renderizar Estructura Ordenada (siempre)
                if structure := file_info.get('structure'):
                    report_lines.append("\n#### Structure:")
                    if not structure:
                        report_lines.append("\n*No significant structure found.*")
                    else:
                        self._render_structure(report_lines, structure, indent_level=0)
                else:
                    report_lines.append("\n*Structure information not available for this file type or parsing failed early.*")

                # Otros datos específicos del parser (menos relevantes, quizás ocultar por defecto?)
                if base_images := file_info.get('base_images'):
                    img_strs = [f"`{img.get('image', 'N/A')}{':' + img.get('tag', '') if img.get('tag') else ''}{'@' + img.get('digest', '') if img.get('digest') else ''}{' as ' + img.get('alias', '') if img.get('alias') else ''}`" for img in base_images]
                    report_lines.append(f"- **(Dockerfile) Base Images:** {', '.join(img_strs)}")
                if exposed_ports := file_info.get('exposed_ports'):
                    report_lines.append(f"- **(Dockerfile) Exposed Ports:** {', '.join(f'`{p}`' for p in exposed_ports)}")


                report_lines.append("\n---\n") # Separador

        # Considerar añadir una nota al final si se usaron filtros
        if not (report_options.summary and report_options.dependencies and report_options.tasks and report_options.file_metadata):
             report_lines.append("\n*Note: Some sections may be hidden. Use flags like `--summary`, `--dependencies`, `--tasks`, `--file-metadata` to show more details.*")

        logger.info("Reporte Markdown generado exitosamente.")
        return "\n".join(report_lines)

    # --- Métodos _render_structure, _render_function, _render_class sin cambios ---
    def _render_structure(self, lines: List[str], structure: List[Dict[str, Any]], indent_level: int = 0):
        """Renderiza recursivamente la lista de elementos estructurales."""
        indent = "  " * indent_level
        for i, element in enumerate(structure):
            elem_type = element.get('type', 'unknown')
            line_start = element.get('line_start', '?')
            if indent_level == 0 and i > 0: 
                lines.append("")
            if elem_type == 'comment':
                lines.append(f"{indent}> *(L{line_start}) {element.get('text', '')}*")
            elif elem_type == 'import':
                lines.append(f"{indent}- *(L{line_start})* Import: `{element.get('value', 'N/A')}`")
            elif elem_type == 'function' or elem_type == 'method':
                self._render_function(lines, element, indent)
            elif elem_type == 'class':
                self._render_class(lines, element, indent, indent_level)
            else:
                lines.append(f"{indent}- *(L{line_start})* **{elem_type.capitalize()}:** `{element.get('name', str(element))}`")

    def _render_function(self, lines: List[str], element: Dict[str, Any], indent: str):
        """Renderiza una función o método."""
        line_start = element.get('line_start', '?')
        name = element.get('name', 'anonymous')
        args = element.get('arguments', [])
        returns = element.get('returns')
        decorators = element.get('decorators', [])
        docstring = element.get('docstring')
        is_async = element.get('is_async', False)
        elem_type = element.get('type', 'function')
        header = f"{elem_type.capitalize()} `{name}`"
        signature = f"({', '.join(args)})"
        if returns: 
            signature += f" -> `{returns}`"
        if is_async:
            header = f"Async {header}"
        lines.append(f"{indent}#### {header} *(L{line_start})*")
        for dec in decorators: 
            lines.append(f"{indent}> Decorator: `{dec}`")
        lines.append(f"{indent}```python\ndef {name}{signature}:\n    ...\n```")
        if docstring:
            lines.append(f"{indent}> **Docstring:**")
            docstring_lines = docstring.strip().split('\n')
            min_indent = float('inf')
            for line in docstring_lines[1:]:
                leading_spaces = len(line) - len(line.lstrip(' '))
                if line.strip(): 
                    min_indent = min(min_indent, leading_spaces)
            if min_indent == float('inf'): 
                min_indent = 0
            formatted_docstring = [docstring_lines[0].strip()]
            formatted_docstring.extend([line[min_indent:] for line in docstring_lines[1:]])
            lines.append(f"{indent}> ```text")
            lines.extend([f"{indent}> {line}" for line in formatted_docstring])
            lines.append(f"{indent}> ```")

    def _render_class(self, lines: List[str], element: Dict[str, Any], indent: str, indent_level: int):
        """Renderiza una clase y su cuerpo."""
        line_start = element.get('line_start', '?')
        name = element.get('name', 'AnonymousClass')
        bases = element.get('bases', [])
        decorators = element.get('decorators', [])
        docstring = element.get('docstring')
        body = element.get('body', [])
        header = f"Class `{name}`"
        if bases: 
            header += f"({', '.join(f'`{b}`' for b in bases)})"
        lines.append(f"{indent}#### {header} *(L{line_start})*")
        for dec in decorators: 
            lines.append(f"{indent}> Decorator: `{dec}`")
        if docstring:
            lines.append(f"{indent}> **Docstring:**")
            docstring_lines = docstring.strip().split('\n')
            min_indent = float('inf')
            for line in docstring_lines[1:]:
                leading_spaces = len(line) - len(line.lstrip(' '))
                if line.strip(): 
                    min_indent = min(min_indent, leading_spaces)
            if min_indent == float('inf'): 
                min_indent = 0
            formatted_docstring = [docstring_lines[0].strip()]
            formatted_docstring.extend([line[min_indent:] for line in docstring_lines[1:]])
            lines.append(f"{indent}> ```text")
            lines.extend([f"{indent}> {line}" for line in formatted_docstring])
            lines.append(f"{indent}> ```")
        if body:
            lines.append(f"{indent}**Body:**")
            self._render_structure(lines, body, indent_level=indent_level + 1)

    # --- Métodos auxiliares _format_size, _append_task_section sin cambios ---
    def _format_size(self, size_bytes: int) -> str:
        if size_bytes < 1024: 
            return f"{size_bytes} B"
        elif size_bytes < 1024 * 1024:
            return f"{size_bytes / 1024:.1f} KB"
        else:
            return f"{size_bytes / (1024 * 1024):.2f} MB"

    # Este método ahora solo se usa si se pide --tasks
    def _append_task_section(self, lines: List[str], title: str, task_map: Dict[str, List[Dict[str, Any]]]):
        lines.append(f"#### {title}")
        if not task_map:
            lines.append(f"- *No {title} found.*")
            lines.append("")
            return
        for file_path, tasks in sorted(task_map.items()):
            lines.append(f"- `{file_path}`:")
            for task in tasks:
                line_num = task.get('line', '?')
                message = task.get('message', 'No message')
                lines.append(f"  - (L{line_num}) {message}")
        lines.append("")
# ==================== FIN: ./reporting/markdown_reporter.py ======================

